{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pip_line.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lksYGkJLVugq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9he2yP4cW9HD",
        "colab_type": "code",
        "outputId": "ecf1f5b7-dfa5-466a-d6eb-793654a98d31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6bx73w_pgx_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43992117-cfe0-400b-fea4-cb34eea15a07"
      },
      "source": [
        "# packages for basic python calculation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# warning ignore\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# packages for fft\n",
        "#import spectrum\n",
        "#from spectrum import Periodogram, data_cosine\n",
        "\n",
        "# packages for ML\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# package for ploting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-talk')\n",
        "\n",
        "from random import sample \n",
        "\n",
        "\n",
        "\n",
        "# defind function for data readin\n",
        "def load_data(folder_path = \"../data/buy/\"):\n",
        "    df_gp = pd.DataFrame()\n",
        "    \n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"): \n",
        "            stock = filename[0:4]\n",
        "            print(\"Loading stock data:\", stock, \",\")\n",
        "            temp_df = pd.read_csv(os.path.join(folder_path, filename), delimiter= '\\s+', header = None)\n",
        "            temp_df.rename(columns={123:'rtn'}, inplace=True)\n",
        "            temp_df = pd.concat([pd.Series([stock] * temp_df.shape[0], name = 'stock'), temp_df], axis=1)\n",
        "            df_gp = pd.concat([df_gp, temp_df], ignore_index=True)\n",
        "            df_gp.reset_index()\n",
        "            continue\n",
        "        else:\n",
        "            continue\n",
        "    \n",
        "    # drop rows with NA\n",
        "    rows_to_drop = []\n",
        "    for i in range(df_gp.shape[0]):\n",
        "        if sum(df_gp.iloc[i,:].isnull()):\n",
        "            rows_to_drop.append(i)\n",
        "    \n",
        "    df_gp = df_gp.drop(rows_to_drop, axis=0)\n",
        "    #df_gp = df_gp.iloc[:,1:]   \n",
        "    \n",
        "    # remove duplicates\n",
        "    df_gp = df_gp.drop_duplicates(subset=df_gp.columns.difference(['stock']))\n",
        "    \n",
        "    # change data type\n",
        "    df_gp.iloc[:, 1:] = df_gp.iloc[:, 1:].astype(float)\n",
        "    \n",
        "    df_gp = df_gp.reset_index(drop = True)\n",
        "    return df_gp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9si_a2JVugx",
        "colab_type": "text"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnRBP6ueVugz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "161ca55b-3cf5-43e3-9ab3-799360eb5f1a"
      },
      "source": [
        "df_gp = load_data(\"/content/gdrive/My Drive/buy/\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading stock data: adbe ,\n",
            "Loading stock data: ions ,\n",
            "Loading stock data: ssys ,\n",
            "Loading stock data: ati1 ,\n",
            "Loading stock data: itub ,\n",
            "Loading stock data: alny ,\n",
            "Loading stock data: pxd1 ,\n",
            "Loading stock data: ufs1 ,\n",
            "Loading stock data: bac1 ,\n",
            "Loading stock data: uri1 ,\n",
            "Loading stock data: clf1 ,\n",
            "Loading stock data: gs1B ,\n",
            "Loading stock data: jnpr ,\n",
            "Loading stock data: crm1 ,\n",
            "Loading stock data: mas1 ,\n",
            "Loading stock data: crus ,\n",
            "Loading stock data: pru1 ,\n",
            "Loading stock data: bidu ,\n",
            "Loading stock data: fslr ,\n",
            "Loading stock data: csiq ,\n",
            "Loading stock data: jec1 ,\n",
            "Loading stock data: db1B ,\n",
            "Loading stock data: acad ,\n",
            "Loading stock data: amd1 ,\n",
            "Loading stock data: cenx ,\n",
            "Loading stock data: teck ,\n",
            "Loading stock data: meli ,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcyLLQT4CyUo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "e9a3a6d2-57fa-45ba-8641-d4fcade6e027"
      },
      "source": [
        "df_gp.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stock</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>rtn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>adbe</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>adbe</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.12</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>adbe</td>\n",
              "      <td>-0.12</td>\n",
              "      <td>-0.13</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.12</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>adbe</td>\n",
              "      <td>-0.15</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>-0.12</td>\n",
              "      <td>-0.13</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.12</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>adbe</td>\n",
              "      <td>-1.36</td>\n",
              "      <td>-1.56</td>\n",
              "      <td>-1.53</td>\n",
              "      <td>-1.34</td>\n",
              "      <td>-0.95</td>\n",
              "      <td>-0.53</td>\n",
              "      <td>0.24</td>\n",
              "      <td>1.05</td>\n",
              "      <td>1.62</td>\n",
              "      <td>2.24</td>\n",
              "      <td>2.68</td>\n",
              "      <td>3.05</td>\n",
              "      <td>3.4</td>\n",
              "      <td>3.64</td>\n",
              "      <td>3.69</td>\n",
              "      <td>3.63</td>\n",
              "      <td>3.38</td>\n",
              "      <td>3.15</td>\n",
              "      <td>2.94</td>\n",
              "      <td>2.45</td>\n",
              "      <td>1.88</td>\n",
              "      <td>1.45</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.14</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.15</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>-0.12</td>\n",
              "      <td>-0.13</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.12</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.27</td>\n",
              "      <td>-0.29</td>\n",
              "      <td>-0.28</td>\n",
              "      <td>-0.18</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.11</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 125 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  stock     0     1     2     3     4  ...  118  119  120  121  122   rtn\n",
              "0  adbe  0.04  0.00  0.00  0.00  0.00  ...  0.0  0.0  0.0  0.0  0.0 -1.55\n",
              "1  adbe -0.11 -0.12 -0.04  0.04  0.00  ...  0.0  0.0  0.0  0.0  0.0 -0.57\n",
              "2  adbe -0.12 -0.13 -0.11 -0.10 -0.11  ...  0.0  0.0  0.0  0.0  0.0  0.00\n",
              "3  adbe -0.15 -0.20 -0.12 -0.13 -0.11  ...  0.0  0.0  0.0  0.0  0.0  5.70\n",
              "4  adbe -1.36 -1.56 -1.53 -1.34 -0.95  ...  0.0  0.0  0.0  0.0  0.0 -1.22\n",
              "\n",
              "[5 rows x 125 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEdX7IIpC0_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# seperate indicators and returns\n",
        "# name_gp = df_gp.iloc[:, 0]\n",
        "osc_gp = df_gp.iloc[:, 1:42]\n",
        "stk_gp = df_gp.iloc[:, 42:83]\n",
        "macd_gp = df_gp.iloc[:, 83:124]\n",
        "rtn_gp = df_gp.iloc[:, 124]\n",
        "label_gp = np.sign(rtn_gp)\n",
        "label_gp = label_gp.map({1: 1, -1: 0, 0:0})\n",
        "results_gp = label_gp.map({1: 'EARN', 0: 'LOSS'})\n",
        "label_gp = pd.DataFrame({\"label\": label_gp})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFH7kTSIFGoy",
        "colab_type": "text"
      },
      "source": [
        "# Convert data to array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeOUtPJpDM0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples_full = []\n",
        "for i in range(osc_gp.shape[0]):\n",
        "  osc_list = osc_gp.iloc[i,:].tolist()\n",
        "  stk_list = stk_gp.iloc[i,:].tolist()\n",
        "  macd_list = macd_gp.iloc[i,:].tolist()\n",
        "  temp_array = np.array((osc_list, stk_list, macd_list), dtype=float)\n",
        "  samples_full.append(temp_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C14JvEtFWz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this cell is delated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiPtJbXcHIOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_y_full = label_gp[\"label\"].tolist()\n",
        "# complete dataset\n",
        "X = np.array(samples_full)\n",
        "y = np.array(sample_y_full)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xgrGn9lHJVl",
        "colab_type": "text"
      },
      "source": [
        "# Train/test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7nfGWhmrRQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.transpose(X, (0,2,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhlJthAnKoaE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f2779ed-b567-4ea2-8152-76755c5729b3"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102408, 41, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXLk4lkQHtQZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "928924dd-c6d9-4a2e-a3ab-29f7a1ce9c38"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102408,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTApF2ak30ED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the index for validation set\n",
        "index_val = sample(list(range(X.shape[0])), int(X.shape[0]*0.2))\n",
        "# get the index for train set\n",
        "index_train = list(set(list(range(X.shape[0]))) - set(index_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZuOV0Wk6RaN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e449826-d740-44a9-bb60-a2102e50caa4"
      },
      "source": [
        "len(index_val)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20481"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkhnwkZL6V6i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1bdc5e2d-4cf8-4e71-9e01-630139a12d21"
      },
      "source": [
        "len(index_train)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81927"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we_pGloy6aOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the training dataset\n",
        "sample_X_train = list(samples_full[i] for i in index_train)\n",
        "sample_y_train = list(sample_y_full[i] for i in index_train)\n",
        "sample_X_train = np.transpose(sample_X_train, (0,2,1))\n",
        "\n",
        "# the test dataset\n",
        "sample_X_val = list(samples_full[i] for i in index_val)\n",
        "sample_y_val = list(sample_y_full[i] for i in index_val)\n",
        "sample_X_val = np.transpose(sample_X_val, (0,2,1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPmREOdU7GrX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.array(sample_X_train)\n",
        "y_train = np.array(sample_y_train)\n",
        "X_val = np.array(sample_X_val)\n",
        "y_val = np.array(sample_y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHdqvJ5xZzuc",
        "colab_type": "text"
      },
      "source": [
        "# Test the array with CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szOIj4ZJFnAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, Conv1D, MaxPooling1D, GlobalAveragePooling2D,BatchNormalization, Conv2D,MaxPooling2D\n",
        "\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam, SGD\n",
        "\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "from keras.utils import np_utils\n",
        "from keras.applications.inception_v3 import InceptionV3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JZ_J7nrFsYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape = (41,3)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(200, activation='relu'))\n",
        "# model.add(Dropout(0.3))\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOngLUtbpkAY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "a34370ae-eeea-4c90-cd36-1373b413c234"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_6 (Conv1D)            (None, 39, 64)            640       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_7 (MaxPooling1 (None, 19, 64)            0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 1216)              0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 200)               243400    \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 200)               40200     \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 284,441\n",
            "Trainable params: 284,441\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am4RNKzdPipL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "dd7a55df-2df8-4206-850e-0e6add34eda3"
      },
      "source": [
        "model.fit(X_train,y_train,\n",
        "      validation_data=(X_val,y_val),\n",
        "      epochs=5,\n",
        "      verbose=True) \n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 81927 samples, validate on 20481 samples\n",
            "Epoch 1/5\n",
            "81927/81927 [==============================] - 20s 244us/step - loss: 0.6949 - acc: 0.5840 - val_loss: 0.6759 - val_acc: 0.5919\n",
            "Epoch 2/5\n",
            "81927/81927 [==============================] - 20s 239us/step - loss: 0.6769 - acc: 0.5891 - val_loss: 0.6758 - val_acc: 0.5920\n",
            "Epoch 3/5\n",
            "81927/81927 [==============================] - 20s 239us/step - loss: 0.6770 - acc: 0.5891 - val_loss: 0.6762 - val_acc: 0.5919\n",
            "Epoch 4/5\n",
            "81927/81927 [==============================] - 20s 244us/step - loss: 0.6762 - acc: 0.5895 - val_loss: 0.6762 - val_acc: 0.5919\n",
            "Epoch 5/5\n",
            "81927/81927 [==============================] - 19s 234us/step - loss: 0.6764 - acc: 0.5897 - val_loss: 0.6757 - val_acc: 0.5919\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f11c4138470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzoYX4Jvsv3a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "76cc286b-8bf9-4640-acf0-68da08a03454"
      },
      "source": [
        "df = pd.DataFrame(model.predict_classes(X_val), columns=[\"pre\"])\n",
        "df[df[\"pre\"] == 1]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13204</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       pre\n",
              "13204    1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtx3-gZD4SLI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23f54a89-178f-4add-889d-1b52dd6989c3"
      },
      "source": [
        "sum(model.predict(X_val) > 0.5)\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTH-nA1CdYPP",
        "colab_type": "text"
      },
      "source": [
        "# The position of the flatten layer and reactive function might matter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jXhtLQCe4sn",
        "colab_type": "text"
      },
      "source": [
        "## Test the optimizer\n",
        "\n",
        "### SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKMJ_ETqdW7n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "outputId": "55a3d66c-9e5d-4c99-bc09-d89beed84089"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape = (41,3)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(200, activation='relu'))\n",
        "# model.add(Dropout(0.3))\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer=\"sgd\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        " \n",
        "\n",
        "model.fit(X_train,y_train,\n",
        "      validation_data=(X_val,y_val),\n",
        "      epochs=5,\n",
        "      verbose=True) \n",
        "\n",
        "df = pd.DataFrame(model.predict_classes(X_val), columns=[\"pre\"])\n",
        "df[df[\"pre\"] >0]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 81927 samples, validate on 20481 samples\n",
            "Epoch 1/5\n",
            "81927/81927 [==============================] - 16s 199us/step - loss: 0.7089 - acc: 0.5871 - val_loss: 0.6760 - val_acc: 0.5919\n",
            "Epoch 2/5\n",
            "81927/81927 [==============================] - 16s 192us/step - loss: 0.6767 - acc: 0.5895 - val_loss: 0.6760 - val_acc: 0.5920\n",
            "Epoch 3/5\n",
            "81927/81927 [==============================] - 15s 188us/step - loss: 0.6765 - acc: 0.5896 - val_loss: 0.6761 - val_acc: 0.5921\n",
            "Epoch 4/5\n",
            "81927/81927 [==============================] - 16s 191us/step - loss: 0.6764 - acc: 0.5893 - val_loss: 0.6760 - val_acc: 0.5921\n",
            "Epoch 5/5\n",
            "81927/81927 [==============================] - 16s 189us/step - loss: 0.6763 - acc: 0.5897 - val_loss: 0.6757 - val_acc: 0.5921\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2064</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4106</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5162</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6296</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8064</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8848</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9150</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12795</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13350</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       pre\n",
              "2064     1\n",
              "4106     1\n",
              "5162     1\n",
              "6296     1\n",
              "8064     1\n",
              "8848     1\n",
              "9150     1\n",
              "12795    1\n",
              "13350    1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "938YBfkOgYnF",
        "colab_type": "text"
      },
      "source": [
        "### Adagrad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMzk6WbRgT93",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "5e7c4f1a-6bcf-4fba-ac9c-e1dc9564edf3"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape = (41,3)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(200, activation='relu'))\n",
        "# model.add(Dropout(0.3))\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer=\"adagrad\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X_train,y_train,\n",
        "      validation_data=(X_val,y_val),\n",
        "      epochs=5,\n",
        "      verbose=True) \n",
        "\n",
        "df = pd.DataFrame(model.predict_classes(X_val), columns=[\"pre\"])\n",
        "df[df[\"pre\"] >0]"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 81927 samples, validate on 20481 samples\n",
            "Epoch 1/5\n",
            "81927/81927 [==============================] - 20s 242us/step - loss: 0.8862 - acc: 0.5825 - val_loss: 0.6773 - val_acc: 0.5920\n",
            "Epoch 2/5\n",
            "81927/81927 [==============================] - 20s 240us/step - loss: 0.6765 - acc: 0.5888 - val_loss: 0.6780 - val_acc: 0.5822\n",
            "Epoch 3/5\n",
            "81927/81927 [==============================] - 20s 242us/step - loss: 0.6760 - acc: 0.5891 - val_loss: 0.6767 - val_acc: 0.5887\n",
            "Epoch 4/5\n",
            "81927/81927 [==============================] - 19s 230us/step - loss: 0.6755 - acc: 0.5892 - val_loss: 0.6759 - val_acc: 0.5915\n",
            "Epoch 5/5\n",
            "81927/81927 [==============================] - 19s 238us/step - loss: 0.6752 - acc: 0.5897 - val_loss: 0.6756 - val_acc: 0.5919\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1113</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2686</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4086</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4106</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8848</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9150</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11082</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12259</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12795</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13350</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       pre\n",
              "1113     1\n",
              "2686     1\n",
              "4086     1\n",
              "4106     1\n",
              "8848     1\n",
              "9150     1\n",
              "11082    1\n",
              "12259    1\n",
              "12795    1\n",
              "13350    1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5-fdm1nhS-A",
        "colab_type": "text"
      },
      "source": [
        "### Nadam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBDJykSihSWQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "e5747b41-e796-44b5-fd2b-3e54f14b078c"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape = (41,3)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(200, activation='relu'))\n",
        "# model.add(Dropout(0.3))\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer=\"nadam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X_train,y_train,\n",
        "      validation_data=(X_val,y_val),\n",
        "      epochs=3,\n",
        "      verbose=True) \n",
        "\n",
        "df = pd.DataFrame(model.predict_classes(X_val), columns=[\"pre\"])\n",
        "df[df[\"pre\"] >0]"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 81927 samples, validate on 20481 samples\n",
            "Epoch 1/3\n",
            "81927/81927 [==============================] - 25s 303us/step - loss: 0.7510 - acc: 0.5861 - val_loss: 0.6768 - val_acc: 0.5916\n",
            "Epoch 2/3\n",
            "81927/81927 [==============================] - 24s 288us/step - loss: 0.6768 - acc: 0.5894 - val_loss: 0.6755 - val_acc: 0.5919\n",
            "Epoch 3/3\n",
            "81927/81927 [==============================] - 24s 293us/step - loss: 0.6769 - acc: 0.5895 - val_loss: 0.6763 - val_acc: 0.5918\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6081</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6296</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      pre\n",
              "6081    1\n",
              "6296    1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS9Ao_Z6huoZ",
        "colab_type": "text"
      },
      "source": [
        "As we can observe from above, changing the optimum function does not changes the acc very much but influence the amount of 1 prediction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhJVivoW8o1p",
        "colab_type": "text"
      },
      "source": [
        "# The upper results look promissing, we could combine the data with the sell data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg1mS0RhiF01",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "b2de9ff0-49d6-465c-bc3e-157357b32917"
      },
      "source": [
        "df_gp_sell = load_data(\"/content/gdrive/My Drive/sell/\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading stock data: fslr ,\n",
            "Loading stock data: csiq ,\n",
            "Loading stock data: bidu ,\n",
            "Loading stock data: pxdS ,\n",
            "Loading stock data: masS ,\n",
            "Loading stock data: uriS ,\n",
            "Loading stock data: ufsS ,\n",
            "Loading stock data: cenx ,\n",
            "Loading stock data: meli ,\n",
            "Loading stock data: pruS ,\n",
            "Loading stock data: lvsS ,\n",
            "Loading stock data: crus ,\n",
            "Loading stock data: crmS ,\n",
            "Loading stock data: itub ,\n",
            "Loading stock data: alny ,\n",
            "Loading stock data: teck ,\n",
            "Loading stock data: dbSe ,\n",
            "Loading stock data: amdS ,\n",
            "Loading stock data: acad ,\n",
            "Loading stock data: adbe ,\n",
            "Loading stock data: ions ,\n",
            "Loading stock data: ssys ,\n",
            "Loading stock data: adsk ,\n",
            "Loading stock data: atiS ,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WINWVppijQpA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "d07d3933-f8a1-453c-de3c-71a4f7bc733b"
      },
      "source": [
        "df_gp_sell.head(5)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stock</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>rtn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fslr</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fslr</td>\n",
              "      <td>-1.72</td>\n",
              "      <td>-1.63</td>\n",
              "      <td>-1.66</td>\n",
              "      <td>-1.61</td>\n",
              "      <td>-1.52</td>\n",
              "      <td>-1.41</td>\n",
              "      <td>-1.30</td>\n",
              "      <td>-1.24</td>\n",
              "      <td>-1.13</td>\n",
              "      <td>-0.98</td>\n",
              "      <td>-0.81</td>\n",
              "      <td>-0.74</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>-0.65</td>\n",
              "      <td>-0.64</td>\n",
              "      <td>-0.60</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.44</td>\n",
              "      <td>-0.38</td>\n",
              "      <td>-0.27</td>\n",
              "      <td>-0.14</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.34</td>\n",
              "      <td>-1.59</td>\n",
              "      <td>-1.60</td>\n",
              "      <td>-1.97</td>\n",
              "      <td>-1.96</td>\n",
              "      <td>-2.24</td>\n",
              "      <td>-2.19</td>\n",
              "      <td>-1.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.27</td>\n",
              "      <td>-0.39</td>\n",
              "      <td>-0.69</td>\n",
              "      <td>-0.57</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.08</td>\n",
              "      <td>-0.31</td>\n",
              "      <td>-0.60</td>\n",
              "      <td>-0.21</td>\n",
              "      <td>-0.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fslr</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>-0.53</td>\n",
              "      <td>-0.79</td>\n",
              "      <td>-1.03</td>\n",
              "      <td>-1.30</td>\n",
              "      <td>-1.47</td>\n",
              "      <td>-1.81</td>\n",
              "      <td>-1.95</td>\n",
              "      <td>-2.10</td>\n",
              "      <td>-2.16</td>\n",
              "      <td>-2.37</td>\n",
              "      <td>-2.64</td>\n",
              "      <td>-2.67</td>\n",
              "      <td>-2.76</td>\n",
              "      <td>-2.63</td>\n",
              "      <td>-2.53</td>\n",
              "      <td>-2.45</td>\n",
              "      <td>-2.32</td>\n",
              "      <td>-2.10</td>\n",
              "      <td>-1.78</td>\n",
              "      <td>-1.72</td>\n",
              "      <td>-1.63</td>\n",
              "      <td>-1.66</td>\n",
              "      <td>-1.61</td>\n",
              "      <td>-1.52</td>\n",
              "      <td>-1.41</td>\n",
              "      <td>-1.30</td>\n",
              "      <td>-1.24</td>\n",
              "      <td>-1.13</td>\n",
              "      <td>-0.98</td>\n",
              "      <td>-0.81</td>\n",
              "      <td>-0.74</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>-0.65</td>\n",
              "      <td>-0.64</td>\n",
              "      <td>-0.60</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.46</td>\n",
              "      <td>-2.93</td>\n",
              "      <td>-3.79</td>\n",
              "      <td>-4.00</td>\n",
              "      <td>-4.57</td>\n",
              "      <td>-3.74</td>\n",
              "      <td>-1.66</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.17</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1.32</td>\n",
              "      <td>1.05</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-1.57</td>\n",
              "      <td>-2.91</td>\n",
              "      <td>-3.73</td>\n",
              "      <td>-4.09</td>\n",
              "      <td>-4.05</td>\n",
              "      <td>-3.10</td>\n",
              "      <td>-1.88</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-1.34</td>\n",
              "      <td>-1.59</td>\n",
              "      <td>-1.60</td>\n",
              "      <td>-1.97</td>\n",
              "      <td>-1.96</td>\n",
              "      <td>-2.24</td>\n",
              "      <td>-2.19</td>\n",
              "      <td>-1.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.27</td>\n",
              "      <td>-0.39</td>\n",
              "      <td>-0.69</td>\n",
              "      <td>-0.57</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fslr</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>-0.53</td>\n",
              "      <td>-0.79</td>\n",
              "      <td>-1.03</td>\n",
              "      <td>-1.30</td>\n",
              "      <td>-1.47</td>\n",
              "      <td>-1.81</td>\n",
              "      <td>-1.95</td>\n",
              "      <td>-2.10</td>\n",
              "      <td>-2.16</td>\n",
              "      <td>-2.37</td>\n",
              "      <td>-2.64</td>\n",
              "      <td>-2.67</td>\n",
              "      <td>-2.76</td>\n",
              "      <td>-2.63</td>\n",
              "      <td>-2.53</td>\n",
              "      <td>-2.45</td>\n",
              "      <td>-2.32</td>\n",
              "      <td>-2.10</td>\n",
              "      <td>-1.78</td>\n",
              "      <td>-1.72</td>\n",
              "      <td>-1.63</td>\n",
              "      <td>-1.66</td>\n",
              "      <td>-1.61</td>\n",
              "      <td>-1.52</td>\n",
              "      <td>-1.41</td>\n",
              "      <td>-1.30</td>\n",
              "      <td>-1.24</td>\n",
              "      <td>-1.13</td>\n",
              "      <td>-0.98</td>\n",
              "      <td>-0.81</td>\n",
              "      <td>-0.74</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>-0.65</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.56</td>\n",
              "      <td>-2.07</td>\n",
              "      <td>-2.46</td>\n",
              "      <td>-2.93</td>\n",
              "      <td>-3.79</td>\n",
              "      <td>-4.00</td>\n",
              "      <td>-4.57</td>\n",
              "      <td>-3.74</td>\n",
              "      <td>-1.66</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.17</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1.32</td>\n",
              "      <td>1.05</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-1.57</td>\n",
              "      <td>-2.91</td>\n",
              "      <td>-3.73</td>\n",
              "      <td>-4.09</td>\n",
              "      <td>-4.05</td>\n",
              "      <td>-3.10</td>\n",
              "      <td>-1.88</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-1.34</td>\n",
              "      <td>-1.59</td>\n",
              "      <td>-1.60</td>\n",
              "      <td>-1.97</td>\n",
              "      <td>-1.96</td>\n",
              "      <td>-2.24</td>\n",
              "      <td>-2.19</td>\n",
              "      <td>-1.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.27</td>\n",
              "      <td>-0.39</td>\n",
              "      <td>-0.69</td>\n",
              "      <td>-0.57</td>\n",
              "      <td>-1.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fslr</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>-0.53</td>\n",
              "      <td>-0.79</td>\n",
              "      <td>-1.03</td>\n",
              "      <td>-1.30</td>\n",
              "      <td>-1.47</td>\n",
              "      <td>-1.81</td>\n",
              "      <td>-1.95</td>\n",
              "      <td>-2.10</td>\n",
              "      <td>-2.16</td>\n",
              "      <td>-2.37</td>\n",
              "      <td>-2.64</td>\n",
              "      <td>-2.67</td>\n",
              "      <td>-2.76</td>\n",
              "      <td>-2.63</td>\n",
              "      <td>-2.53</td>\n",
              "      <td>-2.45</td>\n",
              "      <td>-2.32</td>\n",
              "      <td>-2.10</td>\n",
              "      <td>-1.78</td>\n",
              "      <td>-1.72</td>\n",
              "      <td>-1.63</td>\n",
              "      <td>-1.66</td>\n",
              "      <td>-1.61</td>\n",
              "      <td>-1.52</td>\n",
              "      <td>...</td>\n",
              "      <td>3.46</td>\n",
              "      <td>2.17</td>\n",
              "      <td>1.35</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.16</td>\n",
              "      <td>-1.27</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>-1.33</td>\n",
              "      <td>-1.08</td>\n",
              "      <td>-1.56</td>\n",
              "      <td>-2.07</td>\n",
              "      <td>-2.46</td>\n",
              "      <td>-2.93</td>\n",
              "      <td>-3.79</td>\n",
              "      <td>-4.00</td>\n",
              "      <td>-4.57</td>\n",
              "      <td>-3.74</td>\n",
              "      <td>-1.66</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.17</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1.32</td>\n",
              "      <td>1.05</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-1.57</td>\n",
              "      <td>-2.91</td>\n",
              "      <td>-3.73</td>\n",
              "      <td>-4.09</td>\n",
              "      <td>-4.05</td>\n",
              "      <td>-3.10</td>\n",
              "      <td>-1.88</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-1.34</td>\n",
              "      <td>-1.59</td>\n",
              "      <td>-1.60</td>\n",
              "      <td>-1.97</td>\n",
              "      <td>-1.96</td>\n",
              "      <td>-2.93</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 125 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  stock     0     1     2     3     4  ...   118   119   120   121   122   rtn\n",
              "0  fslr -0.02  0.00  0.00  0.00  0.00  ...  0.00  0.00  0.00  0.00  0.00  0.12\n",
              "1  fslr -1.72 -1.63 -1.66 -1.61 -1.52  ...  0.00  0.00  0.00  0.00  0.00  1.49\n",
              "2  fslr  0.05  0.15 -0.04 -0.17 -0.53  ... -0.39 -0.69 -0.57  0.12  0.08  0.06\n",
              "3  fslr  0.05  0.10  0.05  0.15 -0.04  ...  0.44  0.27 -0.39 -0.69 -0.57 -1.11\n",
              "4  fslr  0.38  0.38  0.37  0.29  0.21  ... -1.34 -1.59 -1.60 -1.97 -1.96 -2.93\n",
              "\n",
              "[5 rows x 125 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojee5cirjW48",
        "colab_type": "text"
      },
      "source": [
        "Here good return is actually represent going down of the stk, the prediction of the model will be focusing on whether the stock is going up or down. Thus, we convert the sign of rtn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIOHOy-6jhmm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "38675c21-15a4-4058-9118-091a361b6c84"
      },
      "source": [
        "df_gp_sell[\"rtn\"] = -1*df_gp_sell[\"rtn\"]\n",
        "df_gp_sell.head()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stock</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>rtn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fslr</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fslr</td>\n",
              "      <td>-1.72</td>\n",
              "      <td>-1.63</td>\n",
              "      <td>-1.66</td>\n",
              "      <td>-1.61</td>\n",
              "      <td>-1.52</td>\n",
              "      <td>-1.41</td>\n",
              "      <td>-1.30</td>\n",
              "      <td>-1.24</td>\n",
              "      <td>-1.13</td>\n",
              "      <td>-0.98</td>\n",
              "      <td>-0.81</td>\n",
              "      <td>-0.74</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>-0.65</td>\n",
              "      <td>-0.64</td>\n",
              "      <td>-0.60</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.44</td>\n",
              "      <td>-0.38</td>\n",
              "      <td>-0.27</td>\n",
              "      <td>-0.14</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.34</td>\n",
              "      <td>-1.59</td>\n",
              "      <td>-1.60</td>\n",
              "      <td>-1.97</td>\n",
              "      <td>-1.96</td>\n",
              "      <td>-2.24</td>\n",
              "      <td>-2.19</td>\n",
              "      <td>-1.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.27</td>\n",
              "      <td>-0.39</td>\n",
              "      <td>-0.69</td>\n",
              "      <td>-0.57</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.08</td>\n",
              "      <td>-0.31</td>\n",
              "      <td>-0.60</td>\n",
              "      <td>-0.21</td>\n",
              "      <td>-0.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-1.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fslr</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>-0.53</td>\n",
              "      <td>-0.79</td>\n",
              "      <td>-1.03</td>\n",
              "      <td>-1.30</td>\n",
              "      <td>-1.47</td>\n",
              "      <td>-1.81</td>\n",
              "      <td>-1.95</td>\n",
              "      <td>-2.10</td>\n",
              "      <td>-2.16</td>\n",
              "      <td>-2.37</td>\n",
              "      <td>-2.64</td>\n",
              "      <td>-2.67</td>\n",
              "      <td>-2.76</td>\n",
              "      <td>-2.63</td>\n",
              "      <td>-2.53</td>\n",
              "      <td>-2.45</td>\n",
              "      <td>-2.32</td>\n",
              "      <td>-2.10</td>\n",
              "      <td>-1.78</td>\n",
              "      <td>-1.72</td>\n",
              "      <td>-1.63</td>\n",
              "      <td>-1.66</td>\n",
              "      <td>-1.61</td>\n",
              "      <td>-1.52</td>\n",
              "      <td>-1.41</td>\n",
              "      <td>-1.30</td>\n",
              "      <td>-1.24</td>\n",
              "      <td>-1.13</td>\n",
              "      <td>-0.98</td>\n",
              "      <td>-0.81</td>\n",
              "      <td>-0.74</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>-0.65</td>\n",
              "      <td>-0.64</td>\n",
              "      <td>-0.60</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.46</td>\n",
              "      <td>-2.93</td>\n",
              "      <td>-3.79</td>\n",
              "      <td>-4.00</td>\n",
              "      <td>-4.57</td>\n",
              "      <td>-3.74</td>\n",
              "      <td>-1.66</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.17</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1.32</td>\n",
              "      <td>1.05</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-1.57</td>\n",
              "      <td>-2.91</td>\n",
              "      <td>-3.73</td>\n",
              "      <td>-4.09</td>\n",
              "      <td>-4.05</td>\n",
              "      <td>-3.10</td>\n",
              "      <td>-1.88</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-1.34</td>\n",
              "      <td>-1.59</td>\n",
              "      <td>-1.60</td>\n",
              "      <td>-1.97</td>\n",
              "      <td>-1.96</td>\n",
              "      <td>-2.24</td>\n",
              "      <td>-2.19</td>\n",
              "      <td>-1.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.27</td>\n",
              "      <td>-0.39</td>\n",
              "      <td>-0.69</td>\n",
              "      <td>-0.57</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.08</td>\n",
              "      <td>-0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>fslr</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>-0.53</td>\n",
              "      <td>-0.79</td>\n",
              "      <td>-1.03</td>\n",
              "      <td>-1.30</td>\n",
              "      <td>-1.47</td>\n",
              "      <td>-1.81</td>\n",
              "      <td>-1.95</td>\n",
              "      <td>-2.10</td>\n",
              "      <td>-2.16</td>\n",
              "      <td>-2.37</td>\n",
              "      <td>-2.64</td>\n",
              "      <td>-2.67</td>\n",
              "      <td>-2.76</td>\n",
              "      <td>-2.63</td>\n",
              "      <td>-2.53</td>\n",
              "      <td>-2.45</td>\n",
              "      <td>-2.32</td>\n",
              "      <td>-2.10</td>\n",
              "      <td>-1.78</td>\n",
              "      <td>-1.72</td>\n",
              "      <td>-1.63</td>\n",
              "      <td>-1.66</td>\n",
              "      <td>-1.61</td>\n",
              "      <td>-1.52</td>\n",
              "      <td>-1.41</td>\n",
              "      <td>-1.30</td>\n",
              "      <td>-1.24</td>\n",
              "      <td>-1.13</td>\n",
              "      <td>-0.98</td>\n",
              "      <td>-0.81</td>\n",
              "      <td>-0.74</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>-0.65</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.56</td>\n",
              "      <td>-2.07</td>\n",
              "      <td>-2.46</td>\n",
              "      <td>-2.93</td>\n",
              "      <td>-3.79</td>\n",
              "      <td>-4.00</td>\n",
              "      <td>-4.57</td>\n",
              "      <td>-3.74</td>\n",
              "      <td>-1.66</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.17</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1.32</td>\n",
              "      <td>1.05</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-1.57</td>\n",
              "      <td>-2.91</td>\n",
              "      <td>-3.73</td>\n",
              "      <td>-4.09</td>\n",
              "      <td>-4.05</td>\n",
              "      <td>-3.10</td>\n",
              "      <td>-1.88</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-1.34</td>\n",
              "      <td>-1.59</td>\n",
              "      <td>-1.60</td>\n",
              "      <td>-1.97</td>\n",
              "      <td>-1.96</td>\n",
              "      <td>-2.24</td>\n",
              "      <td>-2.19</td>\n",
              "      <td>-1.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.27</td>\n",
              "      <td>-0.39</td>\n",
              "      <td>-0.69</td>\n",
              "      <td>-0.57</td>\n",
              "      <td>1.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fslr</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>-0.53</td>\n",
              "      <td>-0.79</td>\n",
              "      <td>-1.03</td>\n",
              "      <td>-1.30</td>\n",
              "      <td>-1.47</td>\n",
              "      <td>-1.81</td>\n",
              "      <td>-1.95</td>\n",
              "      <td>-2.10</td>\n",
              "      <td>-2.16</td>\n",
              "      <td>-2.37</td>\n",
              "      <td>-2.64</td>\n",
              "      <td>-2.67</td>\n",
              "      <td>-2.76</td>\n",
              "      <td>-2.63</td>\n",
              "      <td>-2.53</td>\n",
              "      <td>-2.45</td>\n",
              "      <td>-2.32</td>\n",
              "      <td>-2.10</td>\n",
              "      <td>-1.78</td>\n",
              "      <td>-1.72</td>\n",
              "      <td>-1.63</td>\n",
              "      <td>-1.66</td>\n",
              "      <td>-1.61</td>\n",
              "      <td>-1.52</td>\n",
              "      <td>...</td>\n",
              "      <td>3.46</td>\n",
              "      <td>2.17</td>\n",
              "      <td>1.35</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.16</td>\n",
              "      <td>-1.27</td>\n",
              "      <td>-1.17</td>\n",
              "      <td>-1.33</td>\n",
              "      <td>-1.08</td>\n",
              "      <td>-1.56</td>\n",
              "      <td>-2.07</td>\n",
              "      <td>-2.46</td>\n",
              "      <td>-2.93</td>\n",
              "      <td>-3.79</td>\n",
              "      <td>-4.00</td>\n",
              "      <td>-4.57</td>\n",
              "      <td>-3.74</td>\n",
              "      <td>-1.66</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.17</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1.32</td>\n",
              "      <td>1.05</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-1.57</td>\n",
              "      <td>-2.91</td>\n",
              "      <td>-3.73</td>\n",
              "      <td>-4.09</td>\n",
              "      <td>-4.05</td>\n",
              "      <td>-3.10</td>\n",
              "      <td>-1.88</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.15</td>\n",
              "      <td>-1.34</td>\n",
              "      <td>-1.59</td>\n",
              "      <td>-1.60</td>\n",
              "      <td>-1.97</td>\n",
              "      <td>-1.96</td>\n",
              "      <td>2.93</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 125 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  stock     0     1     2     3     4  ...   118   119   120   121   122   rtn\n",
              "0  fslr -0.02  0.00  0.00  0.00  0.00  ...  0.00  0.00  0.00  0.00  0.00 -0.12\n",
              "1  fslr -1.72 -1.63 -1.66 -1.61 -1.52  ...  0.00  0.00  0.00  0.00  0.00 -1.49\n",
              "2  fslr  0.05  0.15 -0.04 -0.17 -0.53  ... -0.39 -0.69 -0.57  0.12  0.08 -0.06\n",
              "3  fslr  0.05  0.10  0.05  0.15 -0.04  ...  0.44  0.27 -0.39 -0.69 -0.57  1.11\n",
              "4  fslr  0.38  0.38  0.37  0.29  0.21  ... -1.34 -1.59 -1.60 -1.97 -1.96  2.93\n",
              "\n",
              "[5 rows x 125 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J3tK7OQkLXw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46374116-90f6-4847-8f80-ccd35ceb8c92"
      },
      "source": [
        "df = pd.concat([df_gp, df_gp_sell]).reset_index(drop = True)\n",
        "print(df.shape, df_gp.shape, df_gp_sell.shape)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(189247, 125) (102408, 125) (86839, 125)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZzViX7XknAs",
        "colab_type": "text"
      },
      "source": [
        "Here, the newly concat dataframe' rtn represents the movement of the stock market data. We can then study the distribution of the rtn values and determine the threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gCF8oAikmXm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "3fe678e2-08da-4869-f836-0fc318d4ad30"
      },
      "source": [
        "df[\"rtn\"].hist(bins = 1000)\n",
        "# the plot is so peaky due to the outliers"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f11bf5b2c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAowAAAGwCAYAAAAjYzSdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 748.8x514.8 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6ycUhw7lISE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "df48b44e-0837-494d-e55e-8be7ea1146c0"
      },
      "source": [
        "df_cleaned = df[abs(df[\"rtn\"]) < 10]\n",
        "df_cleaned[\"rtn\"].hist(bins = 100)\n",
        "# it looks like a normal dist?\n",
        "# i would use the mean +/- 1/2sd as the two threshold for up, unsure, and down\n",
        "thre_up = df_cleaned[\"rtn\"].mean() + 0.5*df_cleaned[\"rtn\"].std()\n",
        "thre_down = df_cleaned[\"rtn\"].mean() - 0.5*df_cleaned[\"rtn\"].std()\n",
        "plt.axvline(x=thre_up, color = \"black\")\n",
        "plt.axvline(x = thre_down, color = \"black\")"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.lines.Line2D at 0x7f11bd4385c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAowAAAGwCAYAAAAjYzSdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAiMAAF0CIwAAXQIjAABd/z8VTL4IWDur6gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 748.8x514.8 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpIyooV8mGZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_down = df[df[\"rtn\"] < thre_down]\n",
        "df_up = df[df[\"rtn\"] > thre_up]\n",
        "df_avg = df[(df[\"rtn\"] < thre_up) &(df[\"rtn\"] > thre_down)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KZvOJZ6l0oG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25ef0b7c-a412-48c2-fa25-09f745a8ae91"
      },
      "source": [
        "print(df_down.shape, df_up.shape, df_avg.shape)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(37100, 125) (36708, 125) (115439, 125)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqVq7D3QpXYs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "7060deb5-7798-4f00-a87b-0816ca9395fe"
      },
      "source": [
        "df_down[\"mov\"] = -1\n",
        "df_up[\"mov\"] = 1\n",
        "df_avg[\"mov\"] = 0\n",
        "\n",
        "df = pd.concat([df_down, df_up, df_avg]).reset_index(drop = True)\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "df = shuffle(df)\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stock</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>rtn</th>\n",
              "      <th>mov</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>38909</th>\n",
              "      <td>ssys</td>\n",
              "      <td>-1.22</td>\n",
              "      <td>-1.24</td>\n",
              "      <td>-1.20</td>\n",
              "      <td>-1.09</td>\n",
              "      <td>-1.04</td>\n",
              "      <td>-1.04</td>\n",
              "      <td>-1.12</td>\n",
              "      <td>-1.13</td>\n",
              "      <td>-1.03</td>\n",
              "      <td>-1.01</td>\n",
              "      <td>-0.98</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>-0.70</td>\n",
              "      <td>-0.51</td>\n",
              "      <td>-0.29</td>\n",
              "      <td>-0.16</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.68</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>-0.21</td>\n",
              "      <td>-0.16</td>\n",
              "      <td>-0.27</td>\n",
              "      <td>-0.35</td>\n",
              "      <td>-0.34</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>-0.33</td>\n",
              "      <td>-0.36</td>\n",
              "      <td>-0.42</td>\n",
              "      <td>-0.44</td>\n",
              "      <td>-0.40</td>\n",
              "      <td>-0.34</td>\n",
              "      <td>-0.29</td>\n",
              "      <td>-0.24</td>\n",
              "      <td>-0.19</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.05</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.92</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135309</th>\n",
              "      <td>meli</td>\n",
              "      <td>1.90</td>\n",
              "      <td>1.85</td>\n",
              "      <td>1.86</td>\n",
              "      <td>1.92</td>\n",
              "      <td>1.83</td>\n",
              "      <td>1.74</td>\n",
              "      <td>1.72</td>\n",
              "      <td>1.63</td>\n",
              "      <td>1.55</td>\n",
              "      <td>1.46</td>\n",
              "      <td>1.32</td>\n",
              "      <td>1.13</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.38</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.34</td>\n",
              "      <td>-0.62</td>\n",
              "      <td>-0.85</td>\n",
              "      <td>-1.09</td>\n",
              "      <td>-1.27</td>\n",
              "      <td>-1.43</td>\n",
              "      <td>-1.45</td>\n",
              "      <td>-1.42</td>\n",
              "      <td>-1.34</td>\n",
              "      <td>-1.36</td>\n",
              "      <td>-1.36</td>\n",
              "      <td>-1.01</td>\n",
              "      <td>-0.74</td>\n",
              "      <td>-0.41</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.76</td>\n",
              "      <td>1.06</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.81</td>\n",
              "      <td>...</td>\n",
              "      <td>1.08</td>\n",
              "      <td>1.03</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.65</td>\n",
              "      <td>1.36</td>\n",
              "      <td>2.17</td>\n",
              "      <td>2.25</td>\n",
              "      <td>2.29</td>\n",
              "      <td>2.08</td>\n",
              "      <td>1.44</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.12</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.02</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.33</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.67</td>\n",
              "      <td>-0.83</td>\n",
              "      <td>-1.06</td>\n",
              "      <td>-1.21</td>\n",
              "      <td>-1.95</td>\n",
              "      <td>-2.40</td>\n",
              "      <td>-2.37</td>\n",
              "      <td>-1.83</td>\n",
              "      <td>-1.27</td>\n",
              "      <td>-0.55</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>-0.14</td>\n",
              "      <td>-0.31</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>-0.22</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7863</th>\n",
              "      <td>uri1</td>\n",
              "      <td>-3.82</td>\n",
              "      <td>-3.86</td>\n",
              "      <td>-3.66</td>\n",
              "      <td>-3.41</td>\n",
              "      <td>-3.10</td>\n",
              "      <td>-3.19</td>\n",
              "      <td>-3.54</td>\n",
              "      <td>-3.43</td>\n",
              "      <td>-3.40</td>\n",
              "      <td>-3.19</td>\n",
              "      <td>-2.77</td>\n",
              "      <td>-2.38</td>\n",
              "      <td>-1.98</td>\n",
              "      <td>-1.59</td>\n",
              "      <td>-0.82</td>\n",
              "      <td>-0.55</td>\n",
              "      <td>-0.36</td>\n",
              "      <td>-0.19</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>-0.53</td>\n",
              "      <td>-1.06</td>\n",
              "      <td>-1.57</td>\n",
              "      <td>-2.13</td>\n",
              "      <td>-2.51</td>\n",
              "      <td>-2.93</td>\n",
              "      <td>-3.54</td>\n",
              "      <td>-3.67</td>\n",
              "      <td>-3.49</td>\n",
              "      <td>-3.28</td>\n",
              "      <td>-2.86</td>\n",
              "      <td>-2.48</td>\n",
              "      <td>-2.12</td>\n",
              "      <td>-1.47</td>\n",
              "      <td>-0.81</td>\n",
              "      <td>-0.32</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.42</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.73</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>-0.19</td>\n",
              "      <td>-0.18</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>-0.26</td>\n",
              "      <td>-0.35</td>\n",
              "      <td>-0.38</td>\n",
              "      <td>-0.32</td>\n",
              "      <td>-0.31</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>-0.32</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.06</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>-0.45</td>\n",
              "      <td>-0.53</td>\n",
              "      <td>-0.60</td>\n",
              "      <td>-0.64</td>\n",
              "      <td>-0.64</td>\n",
              "      <td>-0.57</td>\n",
              "      <td>-0.47</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.05</td>\n",
              "      <td>-2.01</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40325</th>\n",
              "      <td>itub</td>\n",
              "      <td>-2.12</td>\n",
              "      <td>-2.17</td>\n",
              "      <td>-2.11</td>\n",
              "      <td>-1.95</td>\n",
              "      <td>-1.78</td>\n",
              "      <td>-1.52</td>\n",
              "      <td>-1.31</td>\n",
              "      <td>-0.93</td>\n",
              "      <td>-0.41</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>-0.14</td>\n",
              "      <td>-0.18</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.35</td>\n",
              "      <td>-0.63</td>\n",
              "      <td>-0.96</td>\n",
              "      <td>-1.13</td>\n",
              "      <td>-1.24</td>\n",
              "      <td>-1.21</td>\n",
              "      <td>-1.37</td>\n",
              "      <td>-1.51</td>\n",
              "      <td>-1.56</td>\n",
              "      <td>-1.24</td>\n",
              "      <td>-0.83</td>\n",
              "      <td>-0.47</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.72</td>\n",
              "      <td>1.18</td>\n",
              "      <td>1.52</td>\n",
              "      <td>1.48</td>\n",
              "      <td>1.40</td>\n",
              "      <td>1.31</td>\n",
              "      <td>1.14</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.25</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>-0.12</td>\n",
              "      <td>-0.15</td>\n",
              "      <td>-0.13</td>\n",
              "      <td>-0.13</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>-0.14</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>-0.19</td>\n",
              "      <td>-0.21</td>\n",
              "      <td>-0.22</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>5.51</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72440</th>\n",
              "      <td>adsk</td>\n",
              "      <td>1.05</td>\n",
              "      <td>1.08</td>\n",
              "      <td>1.05</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.95</td>\n",
              "      <td>1.09</td>\n",
              "      <td>1.26</td>\n",
              "      <td>1.42</td>\n",
              "      <td>1.52</td>\n",
              "      <td>1.48</td>\n",
              "      <td>1.46</td>\n",
              "      <td>1.40</td>\n",
              "      <td>1.36</td>\n",
              "      <td>1.34</td>\n",
              "      <td>1.32</td>\n",
              "      <td>1.28</td>\n",
              "      <td>1.29</td>\n",
              "      <td>1.36</td>\n",
              "      <td>1.37</td>\n",
              "      <td>1.36</td>\n",
              "      <td>1.22</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.24</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.19</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>-0.16</td>\n",
              "      <td>-0.51</td>\n",
              "      <td>-0.65</td>\n",
              "      <td>-0.79</td>\n",
              "      <td>-0.72</td>\n",
              "      <td>-0.59</td>\n",
              "      <td>-0.61</td>\n",
              "      <td>-0.66</td>\n",
              "      <td>-0.89</td>\n",
              "      <td>-1.22</td>\n",
              "      <td>-1.26</td>\n",
              "      <td>-1.27</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>-0.56</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.65</td>\n",
              "      <td>1.36</td>\n",
              "      <td>2.05</td>\n",
              "      <td>2.64</td>\n",
              "      <td>2.86</td>\n",
              "      <td>3.18</td>\n",
              "      <td>3.27</td>\n",
              "      <td>2.77</td>\n",
              "      <td>2.04</td>\n",
              "      <td>1.35</td>\n",
              "      <td>1.89</td>\n",
              "      <td>2.31</td>\n",
              "      <td>2.63</td>\n",
              "      <td>2.90</td>\n",
              "      <td>2.97</td>\n",
              "      <td>2.87</td>\n",
              "      <td>2.84</td>\n",
              "      <td>2.44</td>\n",
              "      <td>2.27</td>\n",
              "      <td>2.22</td>\n",
              "      <td>14.86</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 126 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       stock     0     1     2     3     4  ...   119   120   121   122    rtn  mov\n",
              "38909   ssys -1.22 -1.24 -1.20 -1.09 -1.04  ...  0.06  0.15  0.15  0.16   0.92    1\n",
              "135309  meli  1.90  1.85  1.86  1.92  1.83  ... -0.20 -0.22  0.15  0.40   0.21    0\n",
              "7863    uri1 -3.82 -3.86 -3.66 -3.41 -3.10  ... -0.25 -0.07 -0.03  0.05  -2.01   -1\n",
              "40325   itub -2.12 -2.17 -2.11 -1.95 -1.78  ... -0.03 -0.05 -0.07 -0.07   5.51    1\n",
              "72440   adsk  1.05  1.08  1.05  0.92  0.82  ...  2.84  2.44  2.27  2.22  14.86    1\n",
              "\n",
              "[5 rows x 126 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDS-BLcSqo9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# because repeating the same code for multiple times is so boring to me...\n",
        "def generating_train_test(df_gp):\n",
        "  \n",
        "  osc_gp = df_gp.iloc[:, 1:42]\n",
        "  stk_gp = df_gp.iloc[:, 42:83]\n",
        "  macd_gp = df_gp.iloc[:, 83:124]\n",
        "  \n",
        "  # convert data to array\n",
        "  samples_full = []\n",
        "  for i in range(osc_gp.shape[0]):\n",
        "    osc_list = osc_gp.iloc[i,:].tolist()\n",
        "    stk_list = stk_gp.iloc[i,:].tolist()\n",
        "    macd_list = macd_gp.iloc[i,:].tolist()\n",
        "    temp_array = np.array((osc_list, stk_list, macd_list), dtype=float)\n",
        "    samples_full.append(temp_array)\n",
        "  \n",
        "  #print(len(samples_full))\n",
        "  print(\"Now the data are converted to arrays\")\n",
        " \n",
        "  # get the index for validation set\n",
        "  index_val = sample(list(range(df_gp.shape[0])), int(df_gp.shape[0]*0.2))\n",
        "  # get the index for train set\n",
        "  index_train = list(set(list(range(df_gp.shape[0]))) - set(index_val))\n",
        "   \n",
        "  #print(\"Now We generated the index for train and validation\")\n",
        "  # get the train and validation\n",
        "  samples_y_full = df_gp[\"mov\"].tolist()\n",
        "  # the training dataset\n",
        "  sample_X_train = list(samples_full[i] for i in index_train)\n",
        "  sample_y_train = list(samples_y_full[i] for i in index_train)\n",
        "  sample_X_train = np.transpose(sample_X_train, (0,2,1))\n",
        "  \n",
        "  print(\"We finished the spliting and transposing of Training dataset\")\n",
        "\n",
        "  # the validation dataset\n",
        "  sample_X_val = list(samples_full[i] for i in index_val)\n",
        "  sample_y_val = list(samples_y_full[i] for i in index_val)\n",
        "  sample_X_val = np.transpose(sample_X_val, (0,2,1))\n",
        "  print(\"We finished the spliting and transposing of validation dataset\")\n",
        "\n",
        "  X_train = np.array(sample_X_train)\n",
        "  y_train = np.array(sample_y_train)\n",
        "  X_val = np.array(sample_X_val)\n",
        "  y_val = np.array(sample_y_val)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk6mCj9pqiqf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9bd1475d-538e-4f84-c12f-be940c6ea83d"
      },
      "source": [
        "X_train, y_train, X_val, y_val = generating_train_test(df)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now the data are converted to arrays\n",
            "We finished the spliting and transposing of Training dataset\n",
            "We finished the spliting and transposing of validation dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNOyi8QMqKhw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "feeb3c77-541a-4509-afcf-723b5da25fad"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape = (41,3)))\n",
        "model.add(MaxPooling1D(pool_size=6))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='tanh'))\n",
        "# model.add(Dropout(0.3))\n",
        "model.add(Dense(50, activation='tanh'))\n",
        "model.add(Dense(1, activation='softmax'))\n",
        "model.compile(optimizer=\"adam\", loss='squared_hinge', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X_train,y_train,\n",
        "      validation_data=(X_val,y_val),\n",
        "      epochs=5,\n",
        "      verbose=True) \n",
        "\n",
        "#df = pd.DataFrame(model.predict_classes(X_val), columns=[\"pre\"])\n",
        "#df[df[\"pre\"] >0]"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 151398 samples, validate on 37849 samples\n",
            "Epoch 1/5\n",
            "151398/151398 [==============================] - 20s 131us/step - loss: 1.3932 - acc: 0.1943 - val_loss: 1.3981 - val_acc: 0.1927\n",
            "Epoch 2/5\n",
            "151398/151398 [==============================] - 18s 118us/step - loss: 1.3932 - acc: 0.1943 - val_loss: 1.3981 - val_acc: 0.1927\n",
            "Epoch 3/5\n",
            "151398/151398 [==============================] - 19s 125us/step - loss: 1.3932 - acc: 0.1943 - val_loss: 1.3981 - val_acc: 0.1927\n",
            "Epoch 4/5\n",
            "151398/151398 [==============================] - 18s 121us/step - loss: 1.3932 - acc: 0.1943 - val_loss: 1.3981 - val_acc: 0.1927\n",
            "Epoch 5/5\n",
            "151398/151398 [==============================] - 18s 121us/step - loss: 1.3932 - acc: 0.1943 - val_loss: 1.3981 - val_acc: 0.1927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f11b75d15c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXSJZ7OF2t3t",
        "colab_type": "text"
      },
      "source": [
        "The model structure that works fine for binary prediction seems does not work at all for multiclass...need some time to train a new structure maybe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ybPHVQE25fW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "743ebc12-510d-4fab-b7b9-7b8b5d4b0580"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape = (41,3)))\n",
        "model.add(MaxPooling1D(pool_size=3))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='tanh'))\n",
        "opt = SGD(lr=0.01, momentum=0.9)\n",
        "model.compile(loss='squared_hinge', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train,y_train,\n",
        "      validation_data=(X_val,y_val),\n",
        "      epochs=3) \n",
        "\n"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 151398 samples, validate on 37849 samples\n",
            "Epoch 1/3\n",
            "151398/151398 [==============================] - 19s 128us/step - loss: 1.3932 - acc: 0.1943 - val_loss: 1.3981 - val_acc: 0.1927\n",
            "Epoch 2/3\n",
            "151398/151398 [==============================] - 17s 115us/step - loss: 1.3932 - acc: 0.1943 - val_loss: 1.3981 - val_acc: 0.1927\n",
            "Epoch 3/3\n",
            "151398/151398 [==============================] - 17s 115us/step - loss: 1.3932 - acc: 0.1943 - val_loss: 1.3981 - val_acc: 0.1927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f11b6bdbcc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4pAm23c6G7a",
        "colab_type": "text"
      },
      "source": [
        "# Introducing the sell data does not really help with the case..what about look sell data alone\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce7jGxNC6GRt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "7f905046-514e-41fd-d8d3-6c6945b9c9a1"
      },
      "source": [
        "df_gp_sell = load_data(\"/content/gdrive/My Drive/sell/\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading stock data: csiq ,\n",
            "Loading stock data: fslr ,\n",
            "Loading stock data: bidu ,\n",
            "Loading stock data: lvsS ,\n",
            "Loading stock data: uriS ,\n",
            "Loading stock data: masS ,\n",
            "Loading stock data: pxdS ,\n",
            "Loading stock data: crmS ,\n",
            "Loading stock data: crus ,\n",
            "Loading stock data: pruS ,\n",
            "Loading stock data: meli ,\n",
            "Loading stock data: cenx ,\n",
            "Loading stock data: ufsS ,\n",
            "Loading stock data: acad ,\n",
            "Loading stock data: amdS ,\n",
            "Loading stock data: dbSe ,\n",
            "Loading stock data: teck ,\n",
            "Loading stock data: alny ,\n",
            "Loading stock data: itub ,\n",
            "Loading stock data: atiS ,\n",
            "Loading stock data: adsk ,\n",
            "Loading stock data: ssys ,\n",
            "Loading stock data: ions ,\n",
            "Loading stock data: adbe ,\n",
            "Loading stock data: gsSe ,\n",
            "Loading stock data: clfS ,\n",
            "Loading stock data: bacS ,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv-5drjE74WL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# because repeating the same code for multiple times is so boring to me...\n",
        "def generating_train_test(df_gp):\n",
        "  \n",
        "  osc_gp = df_gp.iloc[:, 1:42]\n",
        "  stk_gp = df_gp.iloc[:, 42:83]\n",
        "  macd_gp = df_gp.iloc[:, 83:124]\n",
        "  rtn_gp = df_gp.iloc[:, 124]\n",
        "  label_gp = np.sign(rtn_gp)\n",
        "  label_gp = label_gp.map({1: 1, -1: 0, 0:0})\n",
        "  results_gp = label_gp.map({1: 'EARN', 0: 'LOSS'})\n",
        "  label_gp = pd.DataFrame({\"label\": label_gp})\n",
        "  \n",
        "  print(\"Starting converting data to arrays\")\n",
        "  # convert data to array\n",
        "  samples_full = []\n",
        "  for i in range(osc_gp.shape[0]):\n",
        "    osc_list = osc_gp.iloc[i,:].tolist()\n",
        "    stk_list = stk_gp.iloc[i,:].tolist()\n",
        "    macd_list = macd_gp.iloc[i,:].tolist()\n",
        "    temp_array = np.array((osc_list, stk_list, macd_list), dtype=float)\n",
        "    samples_full.append(temp_array)\n",
        "  \n",
        "  #print(len(samples_full))\n",
        "  print(\"Now the data are converted to arrays\")\n",
        " \n",
        "  # get the index for validation set\n",
        "  index_val = sample(list(range(df_gp.shape[0])), int(df_gp.shape[0]*0.2))\n",
        "  # get the index for train set\n",
        "  index_train = list(set(list(range(df_gp.shape[0]))) - set(index_val))\n",
        "   \n",
        "  print(\"Now We generated the index for train and validation\")\n",
        "  # get the train and validation\n",
        "  sample_y_full = label_gp[\"label\"].tolist()\n",
        "  # the training dataset\n",
        "  sample_X_train = list(samples_full[i] for i in index_train)\n",
        "  sample_y_train = list(sample_y_full[i] for i in index_train)\n",
        "  sample_X_train = np.transpose(sample_X_train, (0,2,1))\n",
        "  \n",
        "  print(\"We finished the spliting and transposing of Training dataset\")\n",
        "\n",
        "  # the validation dataset\n",
        "  sample_X_val = list(samples_full[i] for i in index_val)\n",
        "  sample_y_val = list(sample_y_full[i] for i in index_val)\n",
        "  sample_X_val = np.transpose(sample_X_val, (0,2,1))\n",
        "  print(\"We finished the spliting and transposing of validation dataset\")\n",
        "\n",
        "  X_train = np.array(sample_X_train)\n",
        "  y_train = np.array(sample_y_train)\n",
        "  X_val = np.array(sample_X_val)\n",
        "  y_val = np.array(sample_y_val)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppQ2RHYf6lML",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e14396be-ab5a-4a94-df09-3a81ad35e862"
      },
      "source": [
        "X_train, y_train, X_val, y_val = generating_train_test(df_gp_sell)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting converting data to arrays\n",
            "Now the data are converted to arrays\n",
            "Now We generated the index for train and validation\n",
            "We finished the spliting and transposing of Training dataset\n",
            "We finished the spliting and transposing of validation dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnZajeMM8LMv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "71a73e3f-b13b-4d83-8c81-1d28888891dd"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape = (41,3)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(200, activation='relu'))\n",
        "# model.add(Dropout(0.3))\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer=\"nadam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X_train,y_train,\n",
        "      validation_data=(X_val,y_val),\n",
        "      epochs=3,\n",
        "      verbose=True) \n",
        "\n",
        "df = pd.DataFrame(model.predict_classes(X_val), columns=[\"pre\"])\n",
        "df[df[\"pre\"] >0]"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 82043 samples, validate on 20510 samples\n",
            "Epoch 1/3\n",
            "82043/82043 [==============================] - 28s 344us/step - loss: 0.7509 - acc: 0.5969 - val_loss: 0.6754 - val_acc: 0.5951\n",
            "Epoch 2/3\n",
            "82043/82043 [==============================] - 25s 305us/step - loss: 0.6738 - acc: 0.6006 - val_loss: 0.6758 - val_acc: 0.5944\n",
            "Epoch 3/3\n",
            "82043/82043 [==============================] - 25s 301us/step - loss: 0.6744 - acc: 0.6007 - val_loss: 0.6755 - val_acc: 0.5945\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2210</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3500</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4566</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4990</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5685</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6502</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7538</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8078</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9543</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11064</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11410</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11587</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11603</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11736</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14089</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15610</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15895</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17282</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17966</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19143</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19665</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19945</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       pre\n",
              "2210     1\n",
              "3500     1\n",
              "4566     1\n",
              "4990     1\n",
              "5685     1\n",
              "6502     1\n",
              "7538     1\n",
              "8078     1\n",
              "9543     1\n",
              "11064    1\n",
              "11410    1\n",
              "11587    1\n",
              "11603    1\n",
              "11736    1\n",
              "14089    1\n",
              "15610    1\n",
              "15895    1\n",
              "17282    1\n",
              "17966    1\n",
              "19143    1\n",
              "19665    1\n",
              "19945    1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8ofTqyp8wC1",
        "colab_type": "text"
      },
      "source": [
        "# conclusion:\n",
        "\n",
        "As demonstrated above, the model works seperately instead of combining."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUmTCVqI8_W9",
        "colab_type": "text"
      },
      "source": [
        "# We then can further explore the predicted results\n",
        "## For Buy data\n",
        "\n",
        "previous results lost due to internet disconnect...poor google colab...so start from reading data again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MzkAg8A89gG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "c2a0df80-f980-4cde-ea23-7f415e39d6bc"
      },
      "source": [
        "data_buy = load_data(\"/content/gdrive/My Drive/buy/\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading stock data: adbe ,\n",
            "Loading stock data: ions ,\n",
            "Loading stock data: ssys ,\n",
            "Loading stock data: ati1 ,\n",
            "Loading stock data: itub ,\n",
            "Loading stock data: alny ,\n",
            "Loading stock data: pxd1 ,\n",
            "Loading stock data: ufs1 ,\n",
            "Loading stock data: bac1 ,\n",
            "Loading stock data: uri1 ,\n",
            "Loading stock data: clf1 ,\n",
            "Loading stock data: gs1B ,\n",
            "Loading stock data: jnpr ,\n",
            "Loading stock data: crm1 ,\n",
            "Loading stock data: mas1 ,\n",
            "Loading stock data: crus ,\n",
            "Loading stock data: pru1 ,\n",
            "Loading stock data: bidu ,\n",
            "Loading stock data: fslr ,\n",
            "Loading stock data: csiq ,\n",
            "Loading stock data: jec1 ,\n",
            "Loading stock data: db1B ,\n",
            "Loading stock data: acad ,\n",
            "Loading stock data: amd1 ,\n",
            "Loading stock data: cenx ,\n",
            "Loading stock data: teck ,\n",
            "Loading stock data: meli ,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M4pHoD6-AhK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "588c0562-c079-459e-bd9d-08d8f86c39a2"
      },
      "source": [
        "X_train, y_train, X_val, y_val = generating_train_test(data_buy)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting converting data to arrays\n",
            "Now the data are converted to arrays\n",
            "Now We generated the index for train and validation\n",
            "We finished the spliting and transposing of Training dataset\n",
            "We finished the spliting and transposing of validation dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DinQTIR4-Rqi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "0bb5a8e2-409b-45d3-daa4-1166c4d21754"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=5, kernel_size=3, activation='relu', input_shape = (41,3)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(200, activation='relu'))\n",
        "# model.add(Dropout(0.3))\n",
        "model.add(Dense(164, activation='relu'))\n",
        "\n",
        "model.add(Dense(1, activation='relu'))\n",
        "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train,y_train,\n",
        "      validation_data=(X_val,y_val),\n",
        "      epochs=5,\n",
        "      verbose=True) "
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 81927 samples, validate on 20481 samples\n",
            "Epoch 1/5\n",
            "81927/81927 [==============================] - 11s 129us/step - loss: 6.5910 - acc: 0.5909 - val_loss: 6.6610 - val_acc: 0.5867\n",
            "Epoch 2/5\n",
            "81927/81927 [==============================] - 9s 115us/step - loss: 6.5929 - acc: 0.5910 - val_loss: 6.6610 - val_acc: 0.5867\n",
            "Epoch 3/5\n",
            "81927/81927 [==============================] - 9s 116us/step - loss: 6.5929 - acc: 0.5910 - val_loss: 6.6610 - val_acc: 0.5867\n",
            "Epoch 4/5\n",
            "81927/81927 [==============================] - 9s 116us/step - loss: 6.5929 - acc: 0.5910 - val_loss: 6.6610 - val_acc: 0.5867\n",
            "Epoch 5/5\n",
            "81927/81927 [==============================] - 9s 116us/step - loss: 6.5929 - acc: 0.5910 - val_loss: 6.6610 - val_acc: 0.5867\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f93daa17c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65hdHehM3SWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT-TLpNSCwV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(model.predict(X_val), columns=[\"pre\"])\n",
        "df[\"true\"] = y_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAXSAJMn68eQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JwPhtym681E",
        "colab_type": "text"
      },
      "source": [
        "# Other things to try"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R1qNgZJWwtJ",
        "colab_type": "text"
      },
      "source": [
        "1. We could explore the influence of data balancing to the train data\n",
        "2. We could explore the possibility of seting class weights\n",
        "3. We could explore the influence of data normalizing to the train data\n",
        "4. We could further explore the results with more detailed classification, greater upper, upper, average, lower, and great low by combining the sell data together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we72wmYEkKsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMUNMGyoXdK8",
        "colab_type": "text"
      },
      "source": [
        "### Data balancing\n",
        "\n",
        "There are more loss than gain in the original dataset if we look"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDe-Ne2tXlhr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "7165a857-ff28-43a8-fe78-106576465b0e"
      },
      "source": [
        "rtn_gp.hist(bins = 100)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f11cbc57748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAowAAAGwCAYAAAAjYzSdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nem4urdEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 748.8x514.8 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9OYbwWxYjio",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "045405f6-2348-475e-e590-02598079eb63"
      },
      "source": [
        "print(\"The percentage of rtn lower than 0 is,\" ,sum(rtn_gp < 0)/rtn_gp.shape[0])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The percentage of rtn lower than 0 is, 0.5548101710803843\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzK_XlpaY3wc",
        "colab_type": "text"
      },
      "source": [
        "Thus we can see there are more returns is lower than 0. We can mannully balance the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLy9jCtOY2ms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "data_buy_pos = data_buy[data_buy[\"rtn\"] > 0]\n",
        "data_buy_neg = data_buy[data_buy[\"rtn\"] <= 0]\n",
        "#print(\"gain len\", data_buy_pos.shape[0])\n",
        "#print(\"loss len\", data_buy_neg.shape[0])\n",
        "\n",
        "data_buy_pos_sub = data_buy_pos\n",
        "data_buy_neg_sub = data_buy_neg.sample(n = data_buy_pos.shape[0])\n",
        "\n",
        "data_balance = data_buy_pos_sub.append(data_buy_neg_sub, ignore_index=True)\n",
        "data_balance = shuffle(data_balance)\n",
        "#data_combine = data_combine.reset_index(drop = True)\n",
        "#data_combine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUk4zVfTZbMT",
        "colab_type": "text"
      },
      "source": [
        "Here, instead of df_gp, we have data_ban as our new dataset and all the classes are balanced.\\\n",
        "\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFDDexRQXcWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# because repeating the same code for multiple times is so boring to me...\n",
        "def generating_train_test(df_gp):\n",
        "  \n",
        "  osc_gp = df_gp.iloc[:, 1:42]\n",
        "  stk_gp = df_gp.iloc[:, 42:83]\n",
        "  macd_gp = df_gp.iloc[:, 83:124]\n",
        "  rtn_gp = df_gp.iloc[:, 124]\n",
        "  label_gp = np.sign(rtn_gp)\n",
        "  label_gp = label_gp.map({1: 1, -1: 0, 0:0})\n",
        "  results_gp = label_gp.map({1: 'EARN', 0: 'LOSS'})\n",
        "  label_gp = pd.DataFrame({\"label\": label_gp})\n",
        "  \n",
        "  print(\"Starting converting data to arrays\")\n",
        "  # convert data to array\n",
        "  samples_full = []\n",
        "  for i in range(osc_gp.shape[0]):\n",
        "    osc_list = osc_gp.iloc[i,:].tolist()\n",
        "    stk_list = stk_gp.iloc[i,:].tolist()\n",
        "    macd_list = macd_gp.iloc[i,:].tolist()\n",
        "    temp_array = np.array((osc_list, stk_list, macd_list), dtype=float)\n",
        "    samples_full.append(temp_array)\n",
        "  \n",
        "  #print(len(samples_full))\n",
        "  print(\"Now the data are converted to arrays\")\n",
        " \n",
        "  # get the index for validation set\n",
        "  index_val = sample(list(range(df_gp.shape[0])), int(df_gp.shape[0]*0.2))\n",
        "  # get the index for train set\n",
        "  index_train = list(set(list(range(df_gp.shape[0]))) - set(index_val))\n",
        "   \n",
        "  print(\"Now We generated the index for train and validation\")\n",
        "  # get the train and validation\n",
        "  sample_y_full = label_gp[\"label\"].tolist()\n",
        "  # the training dataset\n",
        "  sample_X_train = list(samples_full[i] for i in index_train)\n",
        "  sample_y_train = list(sample_y_full[i] for i in index_train)\n",
        "  sample_X_train = np.transpose(sample_X_train, (0,2,1))\n",
        "  \n",
        "  print(\"We finished the spliting and transposing of Training dataset\")\n",
        "\n",
        "  # the validation dataset\n",
        "  sample_X_val = list(samples_full[i] for i in index_val)\n",
        "  sample_y_val = list(sample_y_full[i] for i in index_val)\n",
        "  # what if we do not use the transpose\n",
        "  sample_X_val = np.transpose(sample_X_val, (0,2,1))\n",
        "  print(\"We finished the spliting and transposing of validation dataset\")\n",
        "\n",
        "  X_train = np.array(sample_X_train)\n",
        "  y_train = np.array(sample_y_train)\n",
        "  X_val = np.array(sample_X_val)\n",
        "  y_val = np.array(sample_y_val)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPvN-GJGbVM7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9e4c9e86-e976-416a-89da-36a5e1ff3363"
      },
      "source": [
        "# for the balanced datase\n",
        "X_train, y_train, X_val, y_val = generating_train_test(data_balance)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting converting data to arrays\n",
            "Now the data are converted to arrays\n",
            "Now We generated the index for train and validation\n",
            "We finished the spliting and transposing of Training dataset\n",
            "We finished the spliting and transposing of validation dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STkiPuAObSzt",
        "colab_type": "text"
      },
      "source": [
        "We will use the same model setting and to explore the difference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHuMup1Vd0Kj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c59ff61c-1f3f-44f1-efd3-e2fa74390c57"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=5, kernel_size=3, activation='relu', input_shape = (41,3)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(200, activation='relu'))\n",
        "# model.add(Dropout(0.3))\n",
        "model.add(Dense(164, activation='relu'))\n",
        "\n",
        "model.add(Dense(1, activation='relu'))\n",
        "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        " \n",
        "\n",
        "model.fit(X_train,y_train,\n",
        "      validation_data=(X_val,y_val),\n",
        "      epochs=5,\n",
        "      verbose=True) "
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 67160 samples, validate on 16790 samples\n",
            "Epoch 1/5\n",
            "67160/67160 [==============================] - 10s 152us/step - loss: 8.0698 - acc: 0.4993 - val_loss: 8.0158 - val_acc: 0.5027\n",
            "Epoch 2/5\n",
            "67160/67160 [==============================] - 8s 123us/step - loss: 8.0698 - acc: 0.4993 - val_loss: 8.0158 - val_acc: 0.5027\n",
            "Epoch 3/5\n",
            "67160/67160 [==============================] - 8s 123us/step - loss: 8.0698 - acc: 0.4993 - val_loss: 8.0158 - val_acc: 0.5027\n",
            "Epoch 4/5\n",
            "67160/67160 [==============================] - 8s 124us/step - loss: 8.0698 - acc: 0.4993 - val_loss: 8.0158 - val_acc: 0.5027\n",
            "Epoch 5/5\n",
            "67160/67160 [==============================] - 8s 122us/step - loss: 8.0698 - acc: 0.4993 - val_loss: 8.0158 - val_acc: 0.5027\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f93d731fdd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "751SLkrKdBiN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "0114bfac-0cd2-4c80-c70b-37d108dc1d92"
      },
      "source": [
        "df = pd.DataFrame(model.predict_classes(X_val), columns=[\"pre\"])\n",
        "df[df[\"pre\"] > 0]\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [pre]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smB_zpUkekF0",
        "colab_type": "text"
      },
      "source": [
        "After balancing the data, the model performance actually goes down...It could be because the original model is more intended to predict 0 instead of 1, and balancing actually reduce the useful data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzAFtwbse394",
        "colab_type": "text"
      },
      "source": [
        "## What about defining different class weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-s1Fc_ae2Ss",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e5b01c47-ade1-494d-d174-10148926e287"
      },
      "source": [
        "# We still using the original dataset (unbalanced one)\n",
        "\n",
        "X_train, y_train, X_val, y_val = generating_train_test(data_buy)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting converting data to arrays\n",
            "Now the data are converted to arrays\n",
            "Now We generated the index for train and validation\n",
            "We finished the spliting and transposing of Training dataset\n",
            "We finished the spliting and transposing of validation dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVWrq68K97XF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(y_train),\n",
        "                                                 y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUU6cKPxfWVW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "d85c21ef-e32c-4550-a847-b20a1cf87ca4"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=164, kernel_size=3, activation='relu', input_shape = (41,3)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "# model.add(Dropout(0.3))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train,y_train,\n",
        "      validation_data=(X_val,y_val),\n",
        "      epochs=10,\n",
        "      batch_size=32,\n",
        "      verbose=True,\n",
        "      class_weight=class_weights) "
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 81927 samples, validate on 20481 samples\n",
            "Epoch 1/10\n",
            "81927/81927 [==============================] - 24s 288us/step - loss: 0.7105 - acc: 0.5800 - val_loss: 0.6774 - val_acc: 0.5897\n",
            "Epoch 2/10\n",
            "81927/81927 [==============================] - 20s 249us/step - loss: 0.6766 - acc: 0.5896 - val_loss: 0.6761 - val_acc: 0.5896\n",
            "Epoch 3/10\n",
            "81927/81927 [==============================] - 20s 247us/step - loss: 0.6764 - acc: 0.5899 - val_loss: 0.6757 - val_acc: 0.5896\n",
            "Epoch 4/10\n",
            "81927/81927 [==============================] - 20s 250us/step - loss: 0.6761 - acc: 0.5896 - val_loss: 0.6764 - val_acc: 0.5898\n",
            "Epoch 5/10\n",
            "81927/81927 [==============================] - 21s 251us/step - loss: 0.6760 - acc: 0.5901 - val_loss: 0.6766 - val_acc: 0.5898\n",
            "Epoch 6/10\n",
            "81927/81927 [==============================] - 20s 250us/step - loss: 0.6759 - acc: 0.5901 - val_loss: 0.6764 - val_acc: 0.5897\n",
            "Epoch 7/10\n",
            "81927/81927 [==============================] - 20s 250us/step - loss: 0.6759 - acc: 0.5900 - val_loss: 0.6760 - val_acc: 0.5895\n",
            "Epoch 8/10\n",
            "81927/81927 [==============================] - 20s 246us/step - loss: 0.6759 - acc: 0.5903 - val_loss: 0.6756 - val_acc: 0.5899\n",
            "Epoch 9/10\n",
            "81927/81927 [==============================] - 21s 251us/step - loss: 0.6756 - acc: 0.5902 - val_loss: 0.6797 - val_acc: 0.5897\n",
            "Epoch 10/10\n",
            "81927/81927 [==============================] - 21s 259us/step - loss: 0.6758 - acc: 0.5902 - val_loss: 0.6763 - val_acc: 0.5897\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f93d5020390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWneUAP4kF7-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1969
        },
        "outputId": "84d33507-9d2f-4d04-b1f1-cb257cfdac9a"
      },
      "source": [
        "df = pd.DataFrame(model.predict_classes(X_val), columns=[\"predication\"])\n",
        "df[\"true\"]\n",
        "df[df[\"predication\"] == 1]\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predication</th>\n",
              "      <th>true</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16757</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16758</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16759</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16760</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16761</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16762</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16763</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16764</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16765</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16766</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16768</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16769</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16770</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16771</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16773</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16774</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16775</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16776</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16777</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16778</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16779</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16780</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16781</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16782</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16783</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16784</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16785</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16786</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16787</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16788</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16364 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       predication  true\n",
              "0                1     1\n",
              "1                1     1\n",
              "2                1     0\n",
              "3                1     0\n",
              "4                1     0\n",
              "5                1     1\n",
              "6                1     0\n",
              "7                1     1\n",
              "8                1     1\n",
              "9                1     0\n",
              "10               1     0\n",
              "11               1     0\n",
              "12               1     1\n",
              "13               1     1\n",
              "14               1     0\n",
              "15               1     0\n",
              "16               1     1\n",
              "17               1     0\n",
              "18               1     0\n",
              "19               1     0\n",
              "20               1     0\n",
              "21               1     0\n",
              "22               1     1\n",
              "23               1     1\n",
              "24               1     0\n",
              "25               1     0\n",
              "26               1     0\n",
              "27               1     0\n",
              "28               1     0\n",
              "29               1     1\n",
              "...            ...   ...\n",
              "16757            1     0\n",
              "16758            1     1\n",
              "16759            1     1\n",
              "16760            1     0\n",
              "16761            1     0\n",
              "16762            1     0\n",
              "16763            1     0\n",
              "16764            1     1\n",
              "16765            1     0\n",
              "16766            1     0\n",
              "16768            1     1\n",
              "16769            1     0\n",
              "16770            1     0\n",
              "16771            1     1\n",
              "16773            1     1\n",
              "16774            1     1\n",
              "16775            1     1\n",
              "16776            1     1\n",
              "16777            1     1\n",
              "16778            1     1\n",
              "16779            1     1\n",
              "16780            1     0\n",
              "16781            1     1\n",
              "16782            1     1\n",
              "16783            1     0\n",
              "16784            1     1\n",
              "16785            1     1\n",
              "16786            1     0\n",
              "16787            1     1\n",
              "16788            1     0\n",
              "\n",
              "[16364 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PmUSljbinIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# explore the prediction distribution\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bpGbSTEgaCf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "cedec88c-c097-4f3a-84cf-05df806a631e"
      },
      "source": [
        "# what if we put more weights in the 1 class\n",
        "class_weight = {0: 1.,\n",
        "                1: 2.}\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape = (41,3)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(200, activation='relu'))\n",
        "# model.add(Dropout(0.3)) \n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train, \n",
        "          validation_data=(X_val,y_val),\n",
        "          epochs=10, \n",
        "          class_weight=class_weights)\n"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 81927 samples, validate on 20481 samples\n",
            "Epoch 1/10\n",
            "81927/81927 [==============================] - 26s 323us/step - loss: 0.7015 - acc: 0.5833 - val_loss: 0.6767 - val_acc: 0.5898\n",
            "Epoch 2/10\n",
            "81927/81927 [==============================] - 24s 290us/step - loss: 0.6772 - acc: 0.5894 - val_loss: 0.6759 - val_acc: 0.5898\n",
            "Epoch 3/10\n",
            "81927/81927 [==============================] - 23s 278us/step - loss: 0.6763 - acc: 0.5901 - val_loss: 0.6758 - val_acc: 0.5898\n",
            "Epoch 4/10\n",
            "81927/81927 [==============================] - 24s 289us/step - loss: 0.6764 - acc: 0.5900 - val_loss: 0.6756 - val_acc: 0.5897\n",
            "Epoch 5/10\n",
            "81927/81927 [==============================] - 24s 293us/step - loss: 0.6763 - acc: 0.5901 - val_loss: 0.6764 - val_acc: 0.5895\n",
            "Epoch 6/10\n",
            "81927/81927 [==============================] - 25s 299us/step - loss: 0.6762 - acc: 0.5901 - val_loss: 0.6757 - val_acc: 0.5896\n",
            "Epoch 7/10\n",
            "81927/81927 [==============================] - 24s 295us/step - loss: 0.6762 - acc: 0.5902 - val_loss: 0.6758 - val_acc: 0.5897\n",
            "Epoch 8/10\n",
            "81927/81927 [==============================] - 24s 297us/step - loss: 0.6760 - acc: 0.5901 - val_loss: 0.6757 - val_acc: 0.5898\n",
            "Epoch 9/10\n",
            "81927/81927 [==============================] - 24s 288us/step - loss: 0.6760 - acc: 0.5900 - val_loss: 0.6759 - val_acc: 0.5898\n",
            "Epoch 10/10\n",
            "81927/81927 [==============================] - 24s 290us/step - loss: 0.6760 - acc: 0.5901 - val_loss: 0.6760 - val_acc: 0.5898\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f93d4d5c860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUkfMPd2qhSB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Conv1d vs.Conv2d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc5Ntm1dHaVm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "3d1623bc-2816-43d1-f888-df88bf7c9782"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape = (41,3)))\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(200, activation='relu'))\n",
        "# model.add(Dropout(0.3)) \n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, \n",
        "          validation_data=(X_val,y_val),\n",
        "          epochs=5, \n",
        "          class_weight=class_weights)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 81927 samples, validate on 20481 samples\n",
            "Epoch 1/5\n",
            "81927/81927 [==============================] - 34s 409us/step - loss: 0.6919 - acc: 0.5885 - val_loss: 0.6773 - val_acc: 0.5896\n",
            "Epoch 2/5\n",
            "81927/81927 [==============================] - 30s 362us/step - loss: 0.6767 - acc: 0.5900 - val_loss: 0.6761 - val_acc: 0.5898\n",
            "Epoch 3/5\n",
            "81927/81927 [==============================] - 29s 360us/step - loss: 0.6766 - acc: 0.5902 - val_loss: 0.6763 - val_acc: 0.5896\n",
            "Epoch 4/5\n",
            "81927/81927 [==============================] - 30s 367us/step - loss: 0.6761 - acc: 0.5902 - val_loss: 0.6755 - val_acc: 0.5897\n",
            "Epoch 5/5\n",
            "81927/81927 [==============================] - 30s 369us/step - loss: 0.6764 - acc: 0.5901 - val_loss: 0.6766 - val_acc: 0.5899\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f93d4717400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f0EeKwQCLbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2d doesnot really work for this case\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64rcTzjUGMM0",
        "colab_type": "text"
      },
      "source": [
        "# Because this is time series, we could also add LSTM layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRyPdwfoGLgn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "01fc9c97-6e01-4c4e-91c6-79d46f05441f"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape = (41,3)))\n",
        "model.add(Conv1D(filters=64, kernel_size=8, activation='relu'))\n",
        "model.add(LSTM(32, return_sequences=True))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(200, activation='relu'))\n",
        "# model.add(Dropout(0.3)) \n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, \n",
        "          validation_data=(X_val,y_val),\n",
        "          epochs=5, \n",
        "          class_weight=class_weights)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 81927 samples, validate on 20481 samples\n",
            "Epoch 1/5\n",
            "81927/81927 [==============================] - 97s 1ms/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.6756 - val_acc: 0.5898\n",
            "Epoch 2/5\n",
            "81927/81927 [==============================] - 93s 1ms/step - loss: 0.6769 - acc: 0.5902 - val_loss: 0.6795 - val_acc: 0.5898\n",
            "Epoch 3/5\n",
            "81927/81927 [==============================] - 92s 1ms/step - loss: 0.6772 - acc: 0.5898 - val_loss: 0.6759 - val_acc: 0.5898\n",
            "Epoch 4/5\n",
            "81927/81927 [==============================] - 93s 1ms/step - loss: 0.6763 - acc: 0.5902 - val_loss: 0.6763 - val_acc: 0.5898\n",
            "Epoch 5/5\n",
            "81927/81927 [==============================] - 93s 1ms/step - loss: 0.6764 - acc: 0.5902 - val_loss: 0.6757 - val_acc: 0.5898\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_82 (Conv1D)           (None, 39, 64)            640       \n",
            "_________________________________________________________________\n",
            "conv1d_83 (Conv1D)           (None, 32, 64)            32832     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 32, 32)            12416     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_43 (MaxPooling (None, 16, 32)            0         \n",
            "_________________________________________________________________\n",
            "flatten_35 (Flatten)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_113 (Dense)            (None, 200)               102600    \n",
            "_________________________________________________________________\n",
            "dense_114 (Dense)            (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 148,689\n",
            "Trainable params: 148,689\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VIn1Fc-INSk",
        "colab_type": "text"
      },
      "source": [
        "Ok, one last thing for today i want to try is **Normalizing** !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeUQOugDIakf",
        "colab_type": "text"
      },
      "source": [
        "# Normalizing\n",
        "\n",
        "Because the next step is concat, and too tired to write the normalizing code again...I just use the class obj defined before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM9BloacIcxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OSC_analyzing_pipeline():\n",
        "    \n",
        "    \"\"\"\n",
        "    This class is designed and modified for the Seahorse program. It is applicable for Random Forest\n",
        "    Classifier originally.\n",
        "    \n",
        "    Attributes:\n",
        "    ----------------------------------------------------------------------------\n",
        "    self.data: the input dataset that we will be using for training and testing\n",
        "    self.model: the classifier we want to explore\n",
        "    self.osc_gp: subset for the oscillator data\n",
        "    * new addition: self.stk: subset for original stock data\n",
        "    self.macd_gp: subset for the macd data\n",
        "    self.rtn_gp: subset for the return values\n",
        "    self.label_gp: the output for training and testing (the reponse variable)\n",
        "    \n",
        "    self.psd\n",
        "    self.smooth\n",
        "    self.first_derivitive_macd\n",
        "    self.second_derivitive_macd\n",
        "    self.first_derivitive_osc\n",
        "    self.second_derivitive_osc\n",
        "    self.partial_smooth\n",
        "    \n",
        "    self.feature_result_df: the feature matrix and label generated from the feature_generator function, it can be used to visual which feature is participated in model training\n",
        "    self.X_train: to visual and further call the training dataset\n",
        "    self.X_test: to visual and further call the test dataset\n",
        "    self.y_train: to visual and further call the training dataset\n",
        "    self.y_test: to visual and further call the test dataset\n",
        "    \n",
        "    self.report: generate the report to compare the winning rate with the original test\n",
        "    The class object also contains all the attributes belongs to the model/classifier originally.\n",
        "    \n",
        "    Functions:\n",
        "    ----------------------------------------------------------------------------\n",
        "    * new addition: EDA_visualization(): function for visualization the record\n",
        "    psd_calculator(): function calculate the fft values, callable with return value to be visualized\n",
        "    smooth_calculator(): function for calculating the smoothness value, already modified and can handle the amplitude changes\n",
        "    derivative_calculator(): function for calculating the derivitive for macd and osc\n",
        "    features_generator(): function to decide which feature can be used in the final model training, the input is True/False for certain feature to be involved. In this function, all three function above will be called\n",
        "    model_train(): training the model based on the feature matrix\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    # define the init\n",
        "    def __init__(self, data):\n",
        "        \"\"\"\n",
        "        Keyword argument:\n",
        "        data -- (df) the input dataset for processing\n",
        "        model -- (ML model) the classifier we are interested of, currently it is just the random forest\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        \n",
        "        # seperate indicators and returns\n",
        "        # name_gp = df_gp.iloc[:, 0]\n",
        "        self.osc_gp = self.data.iloc[:, 1:42]\n",
        "        self.stk_gp = self.data.iloc[:, 42:83]\n",
        "        self.macd_gp = self.data.iloc[:, 83:124]\n",
        "        self.rtn_gp = self.data.iloc[:, 124]\n",
        "        self.label_gp = np.sign(self.rtn_gp)\n",
        "        self.label_gp = self.label_gp.map({1: 1, -1: 0, 0:0})\n",
        "        self.results_gp = self.label_gp.map({1: 'EARN', 0: 'LOSS'})\n",
        "        self.label_gp = pd.DataFrame({\"label\": self.label_gp})\n",
        "        \n",
        "    \n",
        "    def EDA_visualization(self, index, osc_ind = True, stk_ind = False, macd_ind = False):\n",
        "        if osc_ind:\n",
        "            # plot Oscillator\n",
        "            plt.figure(figsize=(12,4))\n",
        "            plt.plot(range(41),  self.osc_gp.iloc[index,:])\n",
        "            plt.gca().invert_xaxis()\n",
        "            plt.axhline(y=0.0, color = \"black\",linestyle='--')\n",
        "            plt.ylabel(\"Oscillator\")\n",
        "            plt.xlabel(\"Time\")\n",
        "            plt.title(\"Oscillator 3-Day Time Series\")\n",
        "            plt.show()\n",
        "        \n",
        "\n",
        "        if stk_ind:\n",
        "            # plot stock price\n",
        "            plt.figure(figsize=(12,4))\n",
        "            plt.plot(range(41), self.stk_gp.iloc[index,:])\n",
        "            plt.legend()\n",
        "            plt.gca().invert_xaxis()\n",
        "            plt.ylabel(\"Stock Price\")\n",
        "            plt.xlabel(\"Time\")\n",
        "            plt.title(\"Stock Price 3-Day Time Series\")\n",
        "            plt.show()\n",
        "    \n",
        "        if macd_ind:\n",
        "        # plot MACD\n",
        "            plt.figure(figsize=(12,4))\n",
        "            plt.plot(range(41),  self.macd_gp.iloc[index,:])\n",
        "            plt.legend()\n",
        "            plt.gca().invert_xaxis()\n",
        "            plt.axhline(y=0.0, color = \"black\",linestyle='--')\n",
        "            plt.ylabel(\"MACD\")\n",
        "            plt.xlabel(\"Time\")\n",
        "            plt.title(\"MACD 3-Day Time Series\")\n",
        "            plt.show()\n",
        "        \n",
        "    # define function for psd calculation\n",
        "    # define function to calculate smoothness more directly\n",
        "    def smooth_calculator(self):\n",
        "        smooth_list = []\n",
        "        for i in range(self.osc_gp.shape[0]):\n",
        "            amp = np.mean(abs(self.osc_gp.iloc[i,:]))\n",
        "            if amp == 0:\n",
        "                smooth_list.append(0)\n",
        "            else:\n",
        "                smooth_list.append(np.var(np.diff(self.osc_gp.iloc[i,:]))/amp)\n",
        "        \n",
        "        smooth = pd.DataFrame(smooth_list, columns=[\"smooth\"]) \n",
        "        smooth.index = self.osc_gp.index\n",
        "\n",
        "        return smooth\n",
        "    \n",
        "    # define the function to explore the influence of partial smoothness\n",
        "    def smooth_partial(self):\n",
        "        # get the nearest 10 points\n",
        "        osc_10 = self.osc_gp.iloc[:,-10:]\n",
        "        # calculate the smoothness of 10 points\n",
        "        smooth_10 = []\n",
        "        for i in range(osc_10.shape[0]):\n",
        "            amp = np.mean(abs(osc_10.iloc[i,:]))\n",
        "            if amp == 0:\n",
        "                smooth_10.append(0)\n",
        "            else:\n",
        "                smooth_10.append(np.var(np.diff(osc_10.iloc[i,:]))/amp)\n",
        "        \n",
        "        \n",
        "        # get the nearest 20 points\n",
        "        osc_20 = self.osc_gp.iloc[:,-20:]\n",
        "        smooth_20 = []\n",
        "        for i in range(osc_20.shape[0]):\n",
        "            amp = np.mean(abs(osc_20.iloc[i,:]))\n",
        "            if amp == 0:\n",
        "                smooth_20.append(0)\n",
        "            else:\n",
        "                smooth_20.append(np.var(np.diff(osc_20.iloc[i,:]))/(np.mean(abs(osc_20.iloc[i,:]))))\n",
        "        \n",
        "        # get the nearest 30 points\n",
        "        osc_30 = self.osc_gp.iloc[:,-30:]\n",
        "        smooth_30 = []\n",
        "        for i in range(osc_30.shape[0]):\n",
        "            amp = np.mean(abs(osc_30.iloc[i,:]))\n",
        "            if amp == 0:\n",
        "                smooth_30.append(0)\n",
        "            else:\n",
        "                smooth_30.append(np.var(np.diff(osc_30.iloc[i,:]))/(np.mean(abs(osc_30.iloc[i,:]))))\n",
        "        \n",
        "        partial_smooth = pd.DataFrame({\"smooth_10\": smooth_10,\n",
        "                                      \"smooth_20\": smooth_20,\n",
        "                                      \"smooth_30\": smooth_30,})\n",
        "        \n",
        "        partial_smooth.index = self.osc_gp.index\n",
        "        return partial_smooth\n",
        "    \n",
        "    def amp_standardize(self, osc_std = True, stk_std = True, macd_std = True):\n",
        "        \n",
        "        # standardize osc\n",
        "        if osc_std == True:\n",
        "            norm_osc = []\n",
        "            mean_amp = []\n",
        "    \n",
        "            for i in range(self.osc_gp.shape[0]):\n",
        "                mean_amp_temp = np.mean(abs(self.osc_gp.iloc[i,:]))\n",
        "                mean_amp.append(mean_amp_temp)\n",
        "                if mean_amp == 0: \n",
        "                    std = lambda x: x*mean_amp_temp\n",
        "                    norm_osc_temp_list = list(map(std, self.osc_gp.iloc[i,:].tolist()))\n",
        "                    norm_osc.append(norm_osc_temp_list)\n",
        "                else:\n",
        "                    std = lambda x: x/mean_amp_temp\n",
        "                    norm_osc_temp_list = list(map(std, self.osc_gp.iloc[i,:].tolist()))\n",
        "                    norm_osc.append(norm_osc_temp_list)\n",
        "            \n",
        "    \n",
        "            col_name = []\n",
        "            for i in range(len(norm_osc[0])):\n",
        "                col_name.append(\"std_osc\"+str(i))\n",
        "            \n",
        "            amp_osc = pd.DataFrame(norm_osc, columns=col_name)\n",
        "            amp_osc[\"mean_amp_osc\"] = mean_amp\n",
        "            amp_osc.index = self.osc_gp.index\n",
        "        \n",
        "        if stk_std == True:\n",
        "            norm_stk = []\n",
        "            mean_amp_stk = []\n",
        "            \n",
        "            for i in range(self.stk_gp.shape[0]):\n",
        "                mean_amp_temp = np.mean(abs(self.stk_gp.iloc[i,:]))\n",
        "                mean_amp_stk.append(mean_amp_temp)\n",
        "                if mean_amp_temp == 0:\n",
        "                    std = lambda x: x*mean_amp_temp\n",
        "                    norm_stk_temp_list = list(map(std, self.stk_gp.iloc[i,:].tolist()))\n",
        "                    norm_stk.append(norm_stk_temp_list)\n",
        "                else:\n",
        "                    std = lambda x: x/mean_amp_temp\n",
        "                    norm_stk_temp_list = list(map(std, self.stk_gp.iloc[i,:].tolist()))\n",
        "                    norm_stk.append(norm_stk_temp_list)\n",
        "                    \n",
        "            col_name = []\n",
        "            for i in range(len(norm_stk[0])):\n",
        "                col_name.append(\"std_stk\"+str(i))\n",
        "            \n",
        "            amp_stk = pd.DataFrame(norm_stk, columns=col_name)\n",
        "            amp_stk[\"mean_amp_stk\"] = mean_amp_stk\n",
        "            amp_stk.index = self.stk_gp.index\n",
        "        \n",
        "        if macd_std == True:\n",
        "            norm_macd = []\n",
        "            mean_amp_macd = []\n",
        "            \n",
        "            for i in range(self.macd_gp.shape[0]):\n",
        "                mean_amp_temp = np.mean(abs(self.macd_gp.iloc[i,:]))\n",
        "                mean_amp_macd.append(mean_amp_temp)\n",
        "                if mean_amp_temp == 0:\n",
        "                    std = lambda x: x*mean_amp_temp\n",
        "                    norm_macd_temp_list = list(map(std, self.macd_gp.iloc[i,:].tolist()))\n",
        "                    norm_macd.append(norm_macd_temp_list)\n",
        "                else:\n",
        "                    std = lambda x: x/mean_amp_temp\n",
        "                    norm_macd_temp_list = list(map(std, self.macd_gp.iloc[i,:].tolist()))\n",
        "                    norm_macd.append(norm_macd_temp_list)\n",
        "                    \n",
        "            col_name = []\n",
        "            for i in range(len(norm_stk[0])):\n",
        "                col_name.append(\"std_macd\"+str(i))\n",
        "            \n",
        "            amp_macd = pd.DataFrame(norm_macd, columns=col_name)\n",
        "            amp_macd[\"mean_amp_macd\"] = mean_amp_macd\n",
        "            amp_macd.index = self.macd_gp.index\n",
        "            \n",
        "        return amp_osc, amp_stk, amp_macd\n",
        "    \n",
        "    def first_derivative_calculator(self, space = 1, name = \"macd\"):\n",
        "        \n",
        "        \n",
        "        if name == \"macd\":\n",
        "            dy = []\n",
        "            for i in range(self.macd_gp.shape[0]):\n",
        "                y = pd.Series(self.macd_gp.iloc[i,:])\n",
        "                temp_dy = list(np.gradient(y, space))\n",
        "                dy.append(temp_dy)\n",
        "    \n",
        "            col_name = []\n",
        "            for i in range(self.macd_gp.shape[1]):\n",
        "                col_name.append(name + \"deriv\"+ str(i))\n",
        "        \n",
        "            deriv_df = pd.DataFrame(dy, columns=col_name) \n",
        "            deriv_df.index = self.macd_gp.index\n",
        "            \n",
        "        if name == \"osc\":\n",
        "            dy = []\n",
        "            for i in range(self.osc_gp.shape[0]):\n",
        "                y = pd.Series(self.osc_gp.iloc[i,:])\n",
        "                temp_dy = list(np.gradient(y, space))\n",
        "                dy.append(temp_dy)\n",
        "    \n",
        "            col_name = []\n",
        "            for i in range(self.osc_gp.shape[1]):\n",
        "                col_name.append(name + \"deriv\"+ str(i))\n",
        "        \n",
        "            deriv_df = pd.DataFrame(dy, columns=col_name) \n",
        "            deriv_df.index = self.osc_gp.index\n",
        "            \n",
        "        if name == \"stk\":\n",
        "            dy = []\n",
        "            for i in range(self.stk_gp.shape[0]):\n",
        "                y = pd.Series(self.stk_gp.iloc[i,:])\n",
        "                temp_dy = list(np.gradient(y, space))\n",
        "                dy.append(temp_dy)\n",
        "    \n",
        "            col_name = []\n",
        "            for i in range(self.stk_gp.shape[1]):\n",
        "                col_name.append(name + \"deriv\"+ str(i))\n",
        "        \n",
        "            deriv_df = pd.DataFrame(dy, columns=col_name) \n",
        "            deriv_df.index = self.stk_gp.index\n",
        "        return deriv_df\n",
        "    \n",
        "    def second_derivative_calculator(self, space = 1, name = \"macd\"):\n",
        "        \n",
        "        if name == \"macd\":\n",
        "            \n",
        "            ddy = []\n",
        "            for i in range(self.first_deriv_macd.shape[0]):\n",
        "                y = pd.Series(self.first_deriv_macd.iloc[i,:])\n",
        "                temp_ddy = list(np.gradient(y, space))\n",
        "                ddy.append(temp_ddy)\n",
        "    \n",
        "            col_name = []\n",
        "            for i in range(self.first_deriv_macd.shape[1]):\n",
        "                col_name.append(name + \"sec_deriv\"+ str(i))\n",
        "        \n",
        "            sec_deriv_df = pd.DataFrame(ddy, columns=col_name) \n",
        "            sec_deriv_df.index = self.first_deriv_macd.index\n",
        "        \n",
        "        if name == \"osc\":\n",
        "            \n",
        "            ddy = []\n",
        "            for i in range(self.first_deriv_osc.shape[0]):\n",
        "                y = pd.Series(self.first_deriv_osc.iloc[i,:])\n",
        "                temp_ddy = list(np.gradient(y, space))\n",
        "                ddy.append(temp_ddy)\n",
        "    \n",
        "            col_name = []\n",
        "            for i in range(self.first_deriv_osc.shape[1]):\n",
        "                col_name.append(name + \"sec_deriv\"+ str(i))\n",
        "        \n",
        "            sec_deriv_df = pd.DataFrame(ddy, columns=col_name) \n",
        "            sec_deriv_df.index = self.first_deriv_osc.index\n",
        "        \n",
        "        \n",
        "        return sec_deriv_df\n",
        "    \n",
        "    def var_stock_price(self):\n",
        "        variance_stk = []\n",
        "        for i in range(self.stk_gp.shape[0]):\n",
        "            variance_stk.append(np.var(self.stk_gp.iloc[i,:]))\n",
        "        \n",
        "        variance = pd.DataFrame(variance_stk, columns=[\"stk_variance\"]) \n",
        "        variance.index = self.stk_gp.index\n",
        "        \n",
        "        return variance\n",
        "    \n",
        "    def features_generator(self,\n",
        "                           base_features = True,\n",
        "                           smooth=True, \n",
        "                           standardize = True, \n",
        "                           curvature=True, \n",
        "                           derivative = True, \n",
        "                           partial_smooth = True, \n",
        "                           var_stk = True):\n",
        "    # Feature order:\n",
        "    # 0-40, osc;\n",
        "    # 41-81, stk;\n",
        "    # 82-122: macd;\n",
        "    # 123-173: freq\n",
        "    # 174: smooth\n",
        "    # 175-215: first derivitive of osc\n",
        "    # 216-256: second derivitive of osc\n",
        "    # 257-297: first derivitive of macd\n",
        "    # 297-338: second derivitive of macd\n",
        "    \n",
        "    \n",
        "        self.stock = pd.DataFrame(self.data.stock)\n",
        "        self.stock = pd.get_dummies(self.stock)\n",
        "        self.stock.index = self.data.stock.index\n",
        "        \n",
        "        Feature_matrix = self.data.iloc[:,1:-1]\n",
        "        Feature_matrix = pd.merge(Feature_matrix, self.stock, left_index=True, right_index=True)\n",
        "\n",
        "\n",
        "        \n",
        "        # print(Feature_matrix)\n",
        "        # Feature_matrix = pd.concat([Feature_matrix, psd_osc])\n",
        "    \n",
        "    # Factor for smoothness\n",
        "        if smooth==True:\n",
        "            print(\"now generating smoothness\")\n",
        "            self.smooth_osc = self.smooth_calculator()\n",
        "            Feature_matrix = pd.merge(Feature_matrix, self.smooth_osc, left_index=True, right_index=True)\n",
        "        \n",
        "        if standardize == True:\n",
        "            print(\"Normalizing the original osc, stk, macd\")\n",
        "            self.std_osc, self.std_stk, self.std_macd = self.amp_standardize()\n",
        "            Feature_matrix = pd.merge(Feature_matrix, self.std_osc, left_index=True, right_index=True)\n",
        "            Feature_matrix = pd.merge(Feature_matrix, self.std_stk, left_index=True, right_index=True)\n",
        "            Feature_matrix = pd.merge(Feature_matrix, self.std_macd, left_index=True, right_index=True)\n",
        "            \n",
        "            \n",
        "        \n",
        "    # Factor for curvature\n",
        "        if curvature == True:\n",
        "            print(\"now generating osc derivitives\")\n",
        "        # calculate the dy for osc (way to study curvature)\n",
        "            self.first_deriv_osc = self.first_derivative_calculator(name = \"osc\")\n",
        "            Feature_matrix = pd.merge(Feature_matrix, self.first_deriv_osc, left_index=True, right_index=True)\n",
        "        # calculate the ddy for osc\n",
        "            self.second_deriv_osc = self.second_derivative_calculator(name = \"osc\")\n",
        "            Feature_matrix = pd.merge(Feature_matrix, self.second_deriv_osc, left_index=True, right_index=True)\n",
        "        \n",
        "    # MACD dirivative\n",
        "        if derivative == True:\n",
        "            print(\"now generating first derivitives\")\n",
        "            # calculate the dy for macd\n",
        "            self.first_deriv_macd = self.first_derivative_calculator(name = \"macd\")\n",
        "            Feature_matrix = pd.merge(Feature_matrix, self.first_deriv_macd, left_index=True, right_index=True)\n",
        "        # calculate the ddy for macd\n",
        "            self.second_deriv_macd = self.second_derivative_calculator(name = \"macd\")\n",
        "            Feature_matrix = pd.merge(Feature_matrix, self.second_deriv_macd, left_index=True, right_index=True)\n",
        "        # Feature_matrix = pd.concat([Feature_matrix, first_deriv_macd,second_deriv_macd])\n",
        "            self.first_deriv_stk = self.first_derivative_calculator(name = \"stk\")\n",
        "        \n",
        "      \n",
        "    \n",
        "    # partial smoothness\n",
        "        if partial_smooth == True:\n",
        "            print(\"now generating partial smoothness\")\n",
        "            self.partial_smooth = self.smooth_partial()\n",
        "            Feature_matrix = pd.merge(Feature_matrix, self.partial_smooth, left_index=True, right_index=True)\n",
        "        \n",
        "         # variance of stk\n",
        "        if var_stk == True:\n",
        "            print(\"now generating stk volatility\")\n",
        "            self.variance_stk = self.var_stock_price()\n",
        "            Feature_matrix = pd.merge(Feature_matrix, self.variance_stk, left_index=True, right_index=True)\n",
        "        \n",
        "        if base_features == False:\n",
        "            Feature_matrix = Feature_matrix.iloc[:,:123]\n",
        "        \n",
        "        self.feature_result_df = pd.merge(Feature_matrix, self.label_gp, left_index=True, right_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuD0tL-XI7bc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_buy_norm = OSC_analyzing_pipeline(data_buy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ezf9531JCJg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "754c37f7-9933-4904-9885-de1976761b3a"
      },
      "source": [
        "data_buy_norm.features_generator(False, False, True, False, False,False, False)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-144-2be5e91ba233>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_buy_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: features_generator() takes from 1 to 8 positional arguments but 9 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK9onFW4My53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "std_macd = data_buy_norm.std_macd\n",
        "std_osc = data_buy_norm.std_osc\n",
        "std_stk = data_buy_norm.std_stk\n",
        "\n",
        "samples_full = []\n",
        "for i in range(std_osc.shape[0]):\n",
        "    osc_list = std_osc.iloc[i,:].tolist()\n",
        "    stk_list = std_stk.iloc[i,:].tolist()\n",
        "    macd_list = std_macd.iloc[i,:].tolist()\n",
        "    temp_array = np.array((osc_list, stk_list, macd_list), dtype=float)\n",
        "    samples_full.append(temp_array)\n",
        "    \n",
        "# get the index for validation set\n",
        "index_val = sample(list(range(std_macd.shape[0])), int(std_macd.shape[0]*0.2))\n",
        "# get the index for train set\n",
        "index_train = list(set(list(range(std_macd.shape[0]))) - set(index_val))\n",
        "\n",
        "\n",
        "# get the train and validation\n",
        "sample_y_full = data_buy_norm.label_gp[\"label\"].tolist()\n",
        "  # the training dataset\n",
        "sample_X_train = list(samples_full[i] for i in index_train)\n",
        "sample_y_train = list(sample_y_full[i] for i in index_train)\n",
        "sample_X_train = np.transpose(sample_X_train, (0,2,1))\n",
        "\n",
        "\n",
        "sample_X_val = list(samples_full[i] for i in index_val)\n",
        "sample_y_val = list(sample_y_full[i] for i in index_val)\n",
        "  # what if we do not use the transpose\n",
        "sample_X_val = np.transpose(sample_X_val, (0,2,1))\n",
        "\n",
        "\n",
        "X_train = np.array(sample_X_train)\n",
        "y_train = np.array(sample_y_train)\n",
        "X_val = np.array(sample_X_val)\n",
        "y_val = np.array(sample_y_val)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlwF0rrvQk9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "8a044a6d-79d7-4c39-fc6b-de747eef894c"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=164, kernel_size=3, activation='relu', input_shape = (42,3)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "# model.add(Dropout(0.3))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train,y_train,\n",
        "      validation_data=(X_val,y_val),\n",
        "      epochs=10,\n",
        "      batch_size=32,\n",
        "      verbose=True) "
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 81927 samples, validate on 20481 samples\n",
            "Epoch 1/10\n",
            "81927/81927 [==============================] - 35s 428us/step - loss: 0.6793 - acc: 0.5890 - val_loss: 0.6772 - val_acc: 0.5865\n",
            "Epoch 2/10\n",
            "81927/81927 [==============================] - 31s 383us/step - loss: 0.6760 - acc: 0.5910 - val_loss: 0.6776 - val_acc: 0.5866\n",
            "Epoch 3/10\n",
            "81927/81927 [==============================] - 30s 367us/step - loss: 0.6755 - acc: 0.5911 - val_loss: 0.6772 - val_acc: 0.5866\n",
            "Epoch 4/10\n",
            "81927/81927 [==============================] - 30s 370us/step - loss: 0.6752 - acc: 0.5910 - val_loss: 0.6770 - val_acc: 0.5866\n",
            "Epoch 5/10\n",
            "81927/81927 [==============================] - 30s 368us/step - loss: 0.6750 - acc: 0.5911 - val_loss: 0.6774 - val_acc: 0.5866\n",
            "Epoch 6/10\n",
            "81927/81927 [==============================] - 28s 347us/step - loss: 0.6748 - acc: 0.5911 - val_loss: 0.6771 - val_acc: 0.5866\n",
            "Epoch 7/10\n",
            "81927/81927 [==============================] - 31s 377us/step - loss: 0.6748 - acc: 0.5911 - val_loss: 0.6777 - val_acc: 0.5866\n",
            "Epoch 8/10\n",
            "81927/81927 [==============================] - 30s 366us/step - loss: 0.6746 - acc: 0.5911 - val_loss: 0.6836 - val_acc: 0.5866\n",
            "Epoch 9/10\n",
            "81927/81927 [==============================] - 29s 354us/step - loss: 0.6746 - acc: 0.5911 - val_loss: 0.6783 - val_acc: 0.5866\n",
            "Epoch 10/10\n",
            "81927/81927 [==============================] - 30s 365us/step - loss: 0.6745 - acc: 0.5909 - val_loss: 0.6775 - val_acc: 0.5866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f93d15eae10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC0PuZYMHym9",
        "colab_type": "text"
      },
      "source": [
        "# No, No, No!!! What about concat????"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7PX5OKQS9vm",
        "colab_type": "text"
      },
      "source": [
        "There are other features we considered in other models, the psd, smoothness, first and second derivitive and so on. We could use those features to build the model seperately and concat them together. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GadYxoUGu1d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e2a2137a-6417-4cfc-d87e-c1da099c7101"
      },
      "source": [
        "data_buy_norm.features_generator()"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "now generating smoothness\n",
            "Normalizing the original osc, stk, macd\n",
            "now generating osc derivitives\n",
            "now generating first derivitives\n",
            "now generating partial smoothness\n",
            "now generating stk volatility\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrZu6ZL1e3HG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a6bc8e06-a92e-4880-c566-85a19b953e31"
      },
      "source": [
        "data_length = data_buy_norm.data.shape[0]\n",
        "# smoothness\n",
        "smoothness = data_buy_norm.smooth_osc\n",
        "\n",
        "print(\"start processing the basic\")\n",
        "# sample for original data\n",
        "samples_full_basic = []\n",
        "for i in range(data_buy_norm.osc_gp.shape[0]):\n",
        "    osc_list = data_buy_norm.osc_gp.iloc[i,:].tolist()\n",
        "    stk_list = data_buy_norm.stk_gp.iloc[i,:].tolist()\n",
        "    macd_list = data_buy_norm.macd_gp.iloc[i,:].tolist()\n",
        "    temp_array = np.array((osc_list, stk_list, macd_list), dtype=float)\n",
        "    samples_full_basic.append(temp_array)\n",
        "\n",
        "\n",
        "print(\"start processing the deriv\")\n",
        "\n",
        "# sample for first derivitives for those three time series\n",
        "samples_full_deriv = []\n",
        "for i in range(data_buy_norm.first_deriv_macd.shape[0]):\n",
        "    osc_list = data_buy_norm.first_deriv_osc.iloc[i,:].tolist()\n",
        "    stk_list = data_buy_norm.first_deriv_stk.iloc[i,:].tolist()\n",
        "    macd_list = data_buy_norm.first_deriv_macd.iloc[i,:].tolist()\n",
        "    temp_array = np.array((osc_list, stk_list, macd_list), dtype=float)\n",
        "    samples_full_deriv.append(temp_array)\n",
        "    \n",
        "print(\"start processing the norm\")\n",
        "\n",
        "# sample for normalizing data    \n",
        "samples_full_norm = []\n",
        "for i in range(data_buy_norm.std_osc.shape[0]):\n",
        "    osc_list = data_buy_norm.std_osc.iloc[i,:].tolist()\n",
        "    stk_list = data_buy_norm.std_stk.iloc[i,:].tolist()\n",
        "    macd_list = data_buy_norm.std_macd.iloc[i,:].tolist()\n",
        "    temp_array = np.array((osc_list, stk_list, macd_list), dtype=float)\n",
        "    samples_full_norm.append(temp_array)\n",
        "    \n",
        "\n"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start processing the basic\n",
            "start processing the deriv\n",
            "start processing the norm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmT5ruWcssfB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "743bf229-604f-4b9b-9530-a622a09cce01"
      },
      "source": [
        "print(\"creating index\")\n",
        "# get the index for validation set\n",
        "index_val = sample(list(range(data_length)), int(data_length*0.2))\n",
        "# get the index for train set\n",
        "index_train = list(set(list(range(data_length))) - set(index_val))\n",
        "\n",
        "\n",
        "print(\"train/test split for y\")\n",
        "# get the train and validation for y\n",
        "sample_y_full = data_buy_norm.label_gp[\"label\"].tolist()\n",
        "sample_y_train = list(sample_y_full[i] for i in index_train)\n",
        "sample_y_val = list(sample_y_full[i] for i in index_val)\n",
        "y_train = np.array(sample_y_train)\n",
        "y_val = np.array(sample_y_val)\n",
        "\n",
        "\n",
        "print(\"train/test split for sample_basic\")                   \n",
        "# the training dataset for original dataset_ basic three factor\n",
        "sample_X_train_basic = list(samples_full_basic[i] for i in index_train)\n",
        "sample_X_train_basic = np.transpose(sample_X_train_basic, (0,2,1))\n",
        "sample_X_val_basic = list(samples_full_basic[i] for i in index_val)\n",
        "# what if we do not use the transpose\n",
        "sample_X_val_basic = np.transpose(sample_X_val_basic, (0,2,1))\n",
        "X_train_basic = np.array(sample_X_train_basic)\n",
        "X_val_basic = np.array(sample_X_val_basic)\n",
        "\n",
        "print(\"train/test spilt for sample_norm\")\n",
        "# the training dataset for normalized dataset_ norm three factor\n",
        "sample_X_train_norm = list(samples_full_norm[i] for i in index_train)\n",
        "sample_X_train_norm = np.transpose(sample_X_train_norm, (0,2,1))\n",
        "sample_X_val_norm = list(samples_full_norm[i] for i in index_val)\n",
        "# what if we do not use the transpose\n",
        "sample_X_val_norm = np.transpose(sample_X_val_norm, (0,2,1))\n",
        "X_train_norm = np.array(sample_X_train_norm)\n",
        "X_val_norm = np.array(sample_X_val_norm)\n",
        "\n",
        "print(\"train/test spilt for sample_deriv\")\n",
        "\n",
        "# # the training dataset for derivative dataset_ deriv three factor\n",
        "sample_X_train_deriv = list(samples_full_deriv[i] for i in index_train)\n",
        "sample_X_train_deriv = np.transpose(sample_X_train_deriv, (0,2,1))\n",
        "sample_X_val_deriv = list(samples_full_deriv[i] for i in index_val)\n",
        "# what if we do not use the transpose\n",
        "sample_X_val_deriv = np.transpose(sample_X_val_deriv, (0,2,1))\n",
        "X_train_deriv = np.array(sample_X_train_deriv)\n",
        "X_val_deriv = np.array(sample_X_val_deriv)\n",
        "\n",
        "\n",
        "print(\"train/test spilt for sample_smooth\")\n",
        "\n",
        "# the train/test for smoothness data  \n",
        "sample_X_train_smooth = smoothness.ix[index_train]\n",
        "\n",
        "sample_X_val_smooth = smoothness.ix[index_val]\n",
        "# what if we do not use the transpose\n",
        "X_train_smooth = np.array(sample_X_train_smooth)\n",
        "X_val_smooth = np.array(sample_X_val_smooth)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating index\n",
            "train/test split for y\n",
            "train/test split for sample_basic\n",
            "train/test spilt for sample_norm\n",
            "train/test spilt for sample_deriv\n",
            "train/test spilt for sample_smooth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj1HK8GnxeP1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf35fe45-c2dd-41b5-abf9-87428f002416"
      },
      "source": [
        "X_val_deriv.shape"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20481, 41, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcbxRPCouKu7",
        "colab_type": "text"
      },
      "source": [
        "Exam model for each feature individually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diJn_mvxuJ-U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "91861274-5747-43d1-ef5d-32ad151c5df6"
      },
      "source": [
        "# for basic\n",
        "convnet_basic_ts = Sequential()\n",
        "convnet_basic_ts.add(Conv1D(filters=164, kernel_size=8, activation='relu', input_shape = (41,3)))\n",
        "convnet_basic_ts.add(MaxPooling1D(pool_size=2))\n",
        "convnet_basic_ts.add(Flatten())\n",
        "convnet_basic_ts.add(Dense(50, activation='relu'))\n",
        "convnet_basic_ts.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "convnet_basic_ts.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "convnet_basic_ts.fit(\n",
        "    X_train_basic,y_train,\n",
        "    validation_data=(X_val_basic,y_val),\n",
        "    epochs=5,\n",
        "verbose=True)\n",
        "\n"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 81927 samples, validate on 20481 samples\n",
            "Epoch 1/5\n",
            "81927/81927 [==============================] - 38s 460us/step - loss: 0.7086 - acc: 0.5832 - val_loss: 0.6798 - val_acc: 0.5879\n",
            "Epoch 2/5\n",
            "81927/81927 [==============================] - 32s 391us/step - loss: 0.6776 - acc: 0.5900 - val_loss: 0.6782 - val_acc: 0.5879\n",
            "Epoch 3/5\n",
            "81927/81927 [==============================] - 35s 433us/step - loss: 0.6764 - acc: 0.5906 - val_loss: 0.6775 - val_acc: 0.5878\n",
            "Epoch 4/5\n",
            "81927/81927 [==============================] - 36s 433us/step - loss: 0.6764 - acc: 0.5905 - val_loss: 0.6778 - val_acc: 0.5868\n",
            "Epoch 5/5\n",
            "81927/81927 [==============================] - 36s 439us/step - loss: 0.6762 - acc: 0.5904 - val_loss: 0.6778 - val_acc: 0.5879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f93ce6da7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DV-LWeZBYSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG0oCTzgwA4w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "b83755a0-ee6d-4a80-f6e7-4a1ad1fd7a1b"
      },
      "source": [
        "# for norm\n",
        "convnet_norm_ts = Sequential()\n",
        "convnet_norm_ts.add(Conv1D(filters=164, kernel_size=8, activation='relu', input_shape = (42,3)))\n",
        "convnet_norm_ts.add(MaxPooling1D(pool_size=2))\n",
        "convnet_norm_ts.add(Flatten())\n",
        "convnet_norm_ts.add(Dense(50, activation='relu'))\n",
        "convnet_norm_ts.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "convnet_norm_ts.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "convnet_norm_ts.fit(\n",
        "    X_train_norm,y_train,\n",
        "    validation_data=(X_val_norm,y_val),\n",
        "    epochs=5,\n",
        "verbose=True)\n",
        "\n"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 81927 samples, validate on 20481 samples\n",
            "Epoch 1/5\n",
            "81927/81927 [==============================] - 38s 470us/step - loss: 0.6783 - acc: 0.5896 - val_loss: 0.6777 - val_acc: 0.5880\n",
            "Epoch 2/5\n",
            "81927/81927 [==============================] - 32s 392us/step - loss: 0.6771 - acc: 0.5906 - val_loss: 0.6782 - val_acc: 0.5880\n",
            "Epoch 3/5\n",
            "81927/81927 [==============================] - 33s 397us/step - loss: 0.6766 - acc: 0.5907 - val_loss: 0.6774 - val_acc: 0.5880\n",
            "Epoch 4/5\n",
            "81927/81927 [==============================] - 31s 381us/step - loss: 0.6765 - acc: 0.5907 - val_loss: 0.6775 - val_acc: 0.5878\n",
            "Epoch 5/5\n",
            "81927/81927 [==============================] - 32s 387us/step - loss: 0.6764 - acc: 0.5907 - val_loss: 0.6774 - val_acc: 0.5879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f93cdaa1d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A-TahtvxJwa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "5bc4c8c8-4908-4b7c-f381-9d0c0edb456e"
      },
      "source": [
        "convnet_deriv_ts = Sequential()\n",
        "convnet_deriv_ts.add(Conv1D(filters=164, kernel_size=8, activation='relu', input_shape = (41,3)))\n",
        "convnet_deriv_ts.add(MaxPooling1D(pool_size=2))\n",
        "convnet_deriv_ts.add(Flatten())\n",
        "convnet_deriv_ts.add(Dense(50, activation='relu'))\n",
        "convnet_deriv_ts.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "convnet_deriv_ts.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "convnet_deriv_ts.fit(\n",
        "    X_train_deriv,y_train,\n",
        "    validation_data=(X_val_deriv,y_val),\n",
        "    epochs=5,\n",
        "verbose=True)\n",
        "\n"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 81927 samples, validate on 20481 samples\n",
            "Epoch 1/5\n",
            "81927/81927 [==============================] - 27s 329us/step - loss: 0.6776 - acc: 0.5901 - val_loss: 0.6771 - val_acc: 0.5879\n",
            "Epoch 2/5\n",
            "81927/81927 [==============================] - 22s 272us/step - loss: 0.6762 - acc: 0.5904 - val_loss: 0.6764 - val_acc: 0.5879\n",
            "Epoch 3/5\n",
            "81927/81927 [==============================] - 23s 275us/step - loss: 0.6755 - acc: 0.5907 - val_loss: 0.6767 - val_acc: 0.5879\n",
            "Epoch 4/5\n",
            "81927/81927 [==============================] - 23s 279us/step - loss: 0.6753 - acc: 0.5907 - val_loss: 0.6761 - val_acc: 0.5879\n",
            "Epoch 5/5\n",
            "81927/81927 [==============================] - 23s 276us/step - loss: 0.6749 - acc: 0.5907 - val_loss: 0.6762 - val_acc: 0.5879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f93cd6dc5f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKvDj71yyG6m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "3e99882a-73b0-487d-937c-b34e802e7455"
      },
      "source": [
        "model_smooth = Sequential()\n",
        "model_smooth.add(Dense(output_dim =224, activation = 'relu', input_dim = 1))\n",
        "#model_smooth.add(Dense(output_dim = 12, activation = \"relu\"))\n",
        "model_smooth.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
        "\n",
        "model_smooth.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "model_smooth.fit(X_train_smooth, y_train,\n",
        "                validation_data = (X_val_smooth, y_val),\n",
        "                epochs = 5,\n",
        "                verbose = True)"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 81927 samples, validate on 20481 samples\n",
            "Epoch 1/5\n",
            "81927/81927 [==============================] - 10s 121us/step - loss: 0.6769 - acc: 0.5905 - val_loss: 0.6777 - val_acc: 0.5879\n",
            "Epoch 2/5\n",
            "81927/81927 [==============================] - 6s 71us/step - loss: 0.6768 - acc: 0.5907 - val_loss: 0.6777 - val_acc: 0.5879\n",
            "Epoch 3/5\n",
            "81927/81927 [==============================] - 6s 71us/step - loss: 0.6767 - acc: 0.5907 - val_loss: 0.6777 - val_acc: 0.5879\n",
            "Epoch 4/5\n",
            "81927/81927 [==============================] - 6s 71us/step - loss: 0.6768 - acc: 0.5907 - val_loss: 0.6777 - val_acc: 0.5879\n",
            "Epoch 5/5\n",
            "81927/81927 [==============================] - 6s 71us/step - loss: 0.6766 - acc: 0.5907 - val_loss: 0.6782 - val_acc: 0.5879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f93ccfab860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY7XygUmzfjA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4fc833eb-f1f3-44ec-a1fb-383a0d84277b"
      },
      "source": [
        "model_smooth.summary()"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_144 (Dense)            (None, 24)                48        \n",
            "_________________________________________________________________\n",
            "dense_145 (Dense)            (None, 1)                 25        \n",
            "=================================================================\n",
            "Total params: 73\n",
            "Trainable params: 73\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xby89o7c3Al_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "233a1bc2-3aa6-413c-ed26-c82d3633841d"
      },
      "source": [
        "# test if following code works\n",
        "convnet_basic_ts_in = Input(shape=(41,3))\n",
        "convnet_basic_ts_dense_1 = Conv1D(filters=164, kernel_size=8, activation='relu', input_shape = (41,3))(convnet_basic_ts_in)\n",
        "convnet_basic_ts_dense_2 = MaxPooling1D(pool_size=2)(convnet_basic_ts_dense_1)\n",
        "convnet_basic_ts_dense_3 = Flatten()(convnet_basic_ts_dense_2)\n",
        "convnet_basic_ts_dense_4 = Dense(50, activation='relu')(convnet_basic_ts_dense_3)\n",
        "convnet_basic_ts_dense_out = Dense(1, activation = \"sigmoid\")(convnet_basic_ts_dense_4)\n",
        "model1 = Model(convnet_basic_ts_in, convnet_basic_ts_dense_out)\n",
        "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model1.fit(X_train_basic, y_train,\n",
        "                validation_data = (X_val_basic, y_val),\n",
        "                epochs = 5,\n",
        "                verbose = True)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 81927 samples, validate on 20481 samples\n",
            "Epoch 1/5\n",
            "81927/81927 [==============================] - 28s 337us/step - loss: 0.6974 - acc: 0.5849 - val_loss: 0.6788 - val_acc: 0.5876\n",
            "Epoch 2/5\n",
            "81927/81927 [==============================] - 22s 271us/step - loss: 0.6769 - acc: 0.5906 - val_loss: 0.6775 - val_acc: 0.5880\n",
            "Epoch 3/5\n",
            "81927/81927 [==============================] - 22s 264us/step - loss: 0.6770 - acc: 0.5905 - val_loss: 0.6773 - val_acc: 0.5881\n",
            "Epoch 4/5\n",
            "81927/81927 [==============================] - 22s 266us/step - loss: 0.6765 - acc: 0.5902 - val_loss: 0.6770 - val_acc: 0.5879\n",
            "Epoch 5/5\n",
            "81927/81927 [==============================] - 23s 275us/step - loss: 0.6763 - acc: 0.5901 - val_loss: 0.6790 - val_acc: 0.5840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f93cc420dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wG_02hPTaT3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "799c29f2-3a97-4096-a52f-dcd5fc540360"
      },
      "source": [
        "from keras.layers.merge import concatenate\n",
        "\n",
        "# Since the upper code works fine\n",
        "# test if the following code will work\n",
        "\n",
        "convnet_basic_ts_in = Input(shape=(41,3))\n",
        "convnet_basic_ts_dense_1 = Conv1D(filters=164, kernel_size=8, activation='relu', input_shape = (41,3))(convnet_basic_ts_in)\n",
        "convnet_basic_ts_dense_2 = MaxPooling1D(pool_size=2)(convnet_basic_ts_dense_1)\n",
        "convnet_basic_ts_dense_3 = Flatten()(convnet_basic_ts_dense_2)\n",
        "convnet_basic_ts_dense_out = Dense(50, activation='relu')(convnet_basic_ts_dense_3)\n",
        "convnet_basic_ts_dense_model = Model(convnet_basic_ts_in, convnet_basic_ts_dense_out)\n",
        "\n",
        "convnet_norm_ts_in = Input(shape=(42,3))\n",
        "convnet_norm_ts_dense_1 = Conv1D(filters=164, kernel_size=8, activation='relu', input_shape = (41,3))(convnet_norm_ts_in)\n",
        "convnet_norm_ts_dense_2 = MaxPooling1D(pool_size=2)(convnet_norm_ts_dense_1)\n",
        "convnet_norm_ts_dense_3 = Flatten()(convnet_norm_ts_dense_2)\n",
        "convnet_norm_ts_dense_out = Dense(50, activation='relu')(convnet_norm_ts_dense_3)\n",
        "convnet_norm_ts_dense_model = Model(convnet_norm_ts_in, convnet_norm_ts_dense_out)\n",
        "\n",
        "convnet_deriv_ts_in = Input(shape=(41,3))\n",
        "convnet_deriv_ts_dense_1 = Conv1D(filters=164, kernel_size=8, activation='relu', input_shape = (41,3))(convnet_deriv_ts_in)\n",
        "convnet_deriv_ts_dense_2 = MaxPooling1D(pool_size=2)(convnet_deriv_ts_dense_1)\n",
        "convnet_deriv_ts_dense_3 = Flatten()(convnet_deriv_ts_dense_2)\n",
        "convnet_deriv_ts_dense_out = Dense(50, activation='relu')(convnet_deriv_ts_dense_3)\n",
        "convnet_deriv_ts_dense_model = Model(convnet_deriv_ts_in, convnet_deriv_ts_dense_out)\n",
        "\n",
        "\n",
        "concatenated = concatenate([convnet_basic_ts_dense_out, convnet_norm_ts_dense_out, convnet_deriv_ts_dense_out])\n",
        "out = Dense(1, activation='sigmoid', name='output_layer')(concatenated)\n",
        "\n",
        "merged_model = Model([convnet_basic_ts_in, convnet_norm_ts_in, convnet_deriv_ts_in], out)\n",
        "merged_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "merged_model.fit([X_train_basic, X_train_norm, X_train_deriv], y=y_train, epochs=10,\n",
        "             verbose=1, validation_split=0.1, shuffle=True)"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 73734 samples, validate on 8193 samples\n",
            "Epoch 1/10\n",
            "73734/73734 [==============================] - 62s 837us/step - loss: 0.6933 - acc: 0.5864 - val_loss: 0.6862 - val_acc: 0.5875\n",
            "Epoch 2/10\n",
            "73734/73734 [==============================] - 56s 760us/step - loss: 0.6769 - acc: 0.5908 - val_loss: 0.6784 - val_acc: 0.5877\n",
            "Epoch 3/10\n",
            "73734/73734 [==============================] - 56s 760us/step - loss: 0.6763 - acc: 0.5910 - val_loss: 0.6769 - val_acc: 0.5877\n",
            "Epoch 4/10\n",
            "73734/73734 [==============================] - 56s 756us/step - loss: 0.6760 - acc: 0.5910 - val_loss: 0.6769 - val_acc: 0.5878\n",
            "Epoch 5/10\n",
            "73734/73734 [==============================] - 56s 760us/step - loss: 0.6763 - acc: 0.5909 - val_loss: 0.6768 - val_acc: 0.5877\n",
            "Epoch 6/10\n",
            "73734/73734 [==============================] - 55s 743us/step - loss: 0.6756 - acc: 0.5911 - val_loss: 0.6769 - val_acc: 0.5877\n",
            "Epoch 7/10\n",
            "73734/73734 [==============================] - 55s 744us/step - loss: 0.6754 - acc: 0.5911 - val_loss: 0.6767 - val_acc: 0.5877\n",
            "Epoch 8/10\n",
            "73734/73734 [==============================] - 54s 729us/step - loss: 0.6754 - acc: 0.5911 - val_loss: 0.6767 - val_acc: 0.5877\n",
            "Epoch 9/10\n",
            "73734/73734 [==============================] - 54s 736us/step - loss: 0.6754 - acc: 0.5908 - val_loss: 0.6771 - val_acc: 0.5877\n",
            "Epoch 10/10\n",
            "73734/73734 [==============================] - 55s 749us/step - loss: 0.6752 - acc: 0.5909 - val_loss: 0.6773 - val_acc: 0.5878\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f93cb45a7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itUHtz7E0mif",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "9c02d56a-d62c-4eb4-dd90-22a3803efb6d"
      },
      "source": [
        "from keras.layers.merge import concatenate\n",
        "\n",
        "# Since the upper code works fine\n",
        "# test if the following code will work\n",
        "\n",
        "# basic ts\n",
        "convnet_basic_ts_in = Input(shape=(41,3))\n",
        "convnet_basic_ts_dense_1 = Conv1D(filters=164, kernel_size=8, activation='relu', input_shape = (41,3))(convnet_basic_ts_in)\n",
        "convnet_basic_ts_dense_2 = MaxPooling1D(pool_size=2)(convnet_basic_ts_dense_1)\n",
        "convnet_basic_ts_dense_3 = Flatten()(convnet_basic_ts_dense_2)\n",
        "convnet_basic_ts_dense_out = Dense(50, activation='relu')(convnet_basic_ts_dense_3)\n",
        "convnet_basic_ts_dense_model = Model(convnet_basic_ts_in, convnet_basic_ts_dense_out)\n",
        "\n",
        "\n",
        "# normalized ts\n",
        "convnet_norm_ts_in = Input(shape=(42,3))\n",
        "convnet_norm_ts_dense_1 = Conv1D(filters=164, kernel_size=8, activation='relu', input_shape = (41,3))(convnet_norm_ts_in)\n",
        "convnet_norm_ts_dense_2 = MaxPooling1D(pool_size=2)(convnet_norm_ts_dense_1)\n",
        "convnet_norm_ts_dense_3 = Flatten()(convnet_norm_ts_dense_2)\n",
        "convnet_norm_ts_dense_out = Dense(50, activation='relu')(convnet_norm_ts_dense_3)\n",
        "convnet_norm_ts_dense_model = Model(convnet_norm_ts_in, convnet_norm_ts_dense_out)\n",
        "\n",
        "# derivitives ts\n",
        "convnet_deriv_ts_in = Input(shape=(41,3))\n",
        "convnet_deriv_ts_dense_1 = Conv1D(filters=164, kernel_size=8, activation='relu', input_shape = (41,3))(convnet_deriv_ts_in)\n",
        "convnet_deriv_ts_dense_2 = MaxPooling1D(pool_size=2)(convnet_deriv_ts_dense_1)\n",
        "convnet_deriv_ts_dense_3 = Flatten()(convnet_deriv_ts_dense_2)\n",
        "convnet_deriv_ts_dense_out = Dense(50, activation='relu')(convnet_deriv_ts_dense_3)\n",
        "convnet_deriv_ts_dense_model = Model(convnet_deriv_ts_in, convnet_deriv_ts_dense_out)\n",
        "\n",
        "\n",
        "\n",
        "concatenated = concatenate([convnet_basic_ts_dense_out, convnet_norm_ts_dense_out, convnet_deriv_ts_dense_out])\n",
        "out = Dense(1, activation='sigmoid', name='output_layer')(concatenated)\n",
        "\n",
        "merged_model = Model([convnet_basic_ts_in, convnet_norm_ts_in, convnet_deriv_ts_in], out)\n",
        "merged_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "merged_model.fit([X_train_basic, X_train_norm, X_train_deriv], y=y_train, epochs=10,\n",
        "             verbose=1, validation_split=0.1, shuffle=True)"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 73734 samples, validate on 8193 samples\n",
            "Epoch 1/10\n",
            "73734/73734 [==============================] - 61s 822us/step - loss: 0.6955 - acc: 0.5862 - val_loss: 0.6909 - val_acc: 0.5870\n",
            "Epoch 2/10\n",
            "73734/73734 [==============================] - 55s 751us/step - loss: 0.6766 - acc: 0.5905 - val_loss: 0.6773 - val_acc: 0.5877\n",
            "Epoch 3/10\n",
            "73734/73734 [==============================] - 55s 741us/step - loss: 0.6764 - acc: 0.5904 - val_loss: 0.6771 - val_acc: 0.5875\n",
            "Epoch 4/10\n",
            "73734/73734 [==============================] - 53s 724us/step - loss: 0.6754 - acc: 0.5908 - val_loss: 0.6769 - val_acc: 0.5875\n",
            "Epoch 5/10\n",
            "73734/73734 [==============================] - 55s 747us/step - loss: 0.6751 - acc: 0.5910 - val_loss: 0.6765 - val_acc: 0.5871\n",
            "Epoch 6/10\n",
            "73734/73734 [==============================] - 55s 750us/step - loss: 0.6747 - acc: 0.5911 - val_loss: 0.6812 - val_acc: 0.5854\n",
            "Epoch 7/10\n",
            "73734/73734 [==============================] - 52s 704us/step - loss: 0.6743 - acc: 0.5918 - val_loss: 0.6778 - val_acc: 0.5856\n",
            "Epoch 8/10\n",
            "73734/73734 [==============================] - 51s 689us/step - loss: 0.6737 - acc: 0.5921 - val_loss: 0.6777 - val_acc: 0.5862\n",
            "Epoch 9/10\n",
            "73734/73734 [==============================] - 53s 712us/step - loss: 0.6732 - acc: 0.5920 - val_loss: 0.6807 - val_acc: 0.5861\n",
            "Epoch 10/10\n",
            "73734/73734 [==============================] - 51s 690us/step - loss: 0.6727 - acc: 0.5921 - val_loss: 0.6783 - val_acc: 0.5865\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f93ca011c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJASqq1S1esW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shamodel_smooth = Sequential()\n",
        "model_smooth.add(Dense(output_dim =224, activation = 'relu', input_dim = 1))\n",
        "#model_smooth.add(Dense(output_dim = 12, activation = \"relu\"))\n",
        "model_smooth.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
        "\n",
        "model_smooth.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "model_smooth.fit(X_train_smooth, y_train,\n",
        "                validation_data = (X_val_smooth, y_val),\n",
        "                epochs = 5,\n",
        "                verbose = True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
